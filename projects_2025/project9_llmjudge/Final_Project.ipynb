{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Title: Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\n",
        "\n",
        "#### Members' Names :\n",
        "Syed Ali Javed    \n",
        "Sameer Ul Haq\n",
        "\n",
        "####  Emails:\n",
        "syedali.javed@torontomu.ca  \n",
        "sulhaq@torontomu.ca"
      ],
      "metadata": {
        "id": "wEbymPr6_68d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction:\n",
        "\n",
        "This paper introduces a new method for evaluating chatbot models using Large Language Models (LLMs) as judges. Specifically, it explores whether GPT-4 — a model trained using human feedback — can fairly and reliably evaluate other chat assistants.\n",
        "\n",
        "To do this, the research introduced:\n",
        "\n",
        "MT-Bench – a benchmark of multi-turn, open-ended questions\n",
        "\n",
        "Chatbot Arena – a platform where users vote between chatbot responses\n",
        "\n",
        "The aim of the research was to replace costly judgment with an AI Judge approach.\n",
        "\n",
        "#### Problem Description:\n",
        "\n",
        "The research tells that current benchmarks, such as MMLU, which test how well a language model knows facts by asking closed-ended questions (like multiple-choice questions). But real conversations with chatbots are open-ended, often with multiple valid answers, and involve multi-turn dialogue.\n",
        "\n",
        "As a result, a chatbot can score high on these old benchmarks but still give poor or unhelpful answers in real conversations. We don’t have a good way to measure how helpful or human-aligned a chatbot really is.\n",
        "\n",
        "\n",
        "#### Context of the Problem:\n",
        "\n",
        "As LLMs are now used for writing, coding, reasoning, and chatting, their evaluation needs to measure how well they match human expectations — not just correctness.\n",
        "\n",
        "Manual evaluation by humans is accurate, but it’s also expensive, slow, and unscalable.\n",
        "\n",
        "LLMs like GPT-4 are trained using Reinforcement Learning from Human Feedback (RLHF), which teaches them to follow human-like instructions. This makes GPT-4 a strong candidate for judging chatbot responses.\n",
        "\n",
        "But this had never been studied in detail. This paper fills that gap.\n",
        "\n",
        "#### Limitation About other Approaches:\n",
        "\n",
        "The authors categorize existing LLM evaluation benchmarks into three groups:\n",
        "\n",
        "Core knowledge – MMLU, ARC, HumanEval. These test factual knowledge using short answers.\n",
        "\n",
        "Instruction-following – Flan, Self-Instruct. These check if a model follows tasks, but lack diversity and realism.\n",
        "\n",
        "Conversational – CoQA, OpenAssistant. These simulate dialogues but lack challenge and don’t scale.\n",
        "\n",
        "All of them fail to assess open-ended dialogue, multi-turn reasoning, or human preferences like helpfulness, clarity, and creativity.\n",
        "\n",
        "Hence, they are inadequate to evaluate modern LLMs in real settings\n",
        "\n",
        "#### Solution:\n",
        "\n",
        "This paper introduces a new method for evaluating chatbot models using Large Language Models (LLMs) as judges. Specifically, it explores whether GPT-4 — a model trained using human feedback — can fairly and reliably evaluate other chat assistants.\n",
        "\n",
        "To do this, the authors introduce:\n",
        "\n",
        "MT-Bench – a benchmark of multi-turn, open-ended questions\n",
        "\n",
        "Chatbot Arena – a platform where users vote between chatbot responses\n",
        "\n",
        "They compare GPT-4's evaluations with human preferences and find that GPT-4 can match expert-level agreement over 80% of the time. This opens a door to scalable and automated chatbot evaluation.\"\n"
      ],
      "metadata": {
        "id": "zd771OMQ_643"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Background\n",
        "\n",
        "\n",
        "| Reference |Explanation |  Dataset/Input |Weakness\n",
        "| --- | --- | --- | --- |\n",
        "| Hendrycks et al., 2020 [1] | Evaluates factual recall through multiple-choice questions | 57 knowledge tasks | Benchmarks like MMLU cannot effectively distinguish between base and aligned models (e.g., GPT-3). They primarily focus on closed-ended questions with short responses.\n",
        "| Reddy, Chen, & Manning, 2019 [2] | Conversational QA benchmark simulating dialogue | CoQA dataset | Conversational benchmarks like CoQA fall short in challenging the capabilities of the latest chatbots.\n",
        "\n"
      ],
      "metadata": {
        "id": "smyfdf25_61_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methodology\n",
        "\n",
        "Benchmarks Introduced:\n",
        "\n",
        "- MT-Bench – 80 carefully designed multi-turn questions in 8 categories:\n",
        "\n",
        "    Writing, Roleplay, Reasoning, Math, Coding, Extraction, STEM, Humanities​\n",
        "\n",
        "    Each question has two turns. Two turns simulate back-and-forth — evaluating not just the first answer but how the model continues the dialogue. Helps test memory, coherence, follow-up accuracy.\n",
        "\n",
        "\n",
        "- Chatbot Arena – A live web platform where users vote between two anonymous chatbot responses. 30K conversations were collected with 3K used for controlled analysis​.\n",
        "\n",
        "\n",
        "Evaluation Setup:\n",
        "\n",
        "The authors test 6 models: GPT-4, GPT-3.5, Claude, Vicuna, Alpaca, LLaMA.\n",
        "They use two judge types:\n",
        "\n",
        "LLM Judges (GPT-4, GPT-3.5, Claude)\n",
        "\n",
        "Humans (58 experts for MT-Bench, 2,114 crowd users for Arena)\n",
        "\n",
        "Three judgment modes:\n",
        "\n",
        "- Pairwise Comparison: GPT-4 sees Q + Answer A + Answer B → Picks best or tie​\n",
        "\n",
        "- Single Answer Grading: GPT-4 scores one answer on 1–10 scale\n",
        "\n",
        "- Reference-Guided: A reference answer is given → GPT-4 compares both to that.\n",
        "\n",
        "\n",
        "\n",
        "Evaluation Results:\n",
        "\n",
        "GPT-4 agrees with expert humans 85% of the time — same as human–human agreement (81%)​\n",
        "\n",
        "GPT-4 also more decisive: fewer tie votes than Claude or GPT-3.5\n",
        "\n",
        "75% of users agreed with GPT-4’s explanation\n",
        "\n",
        "34% changed their vote after seeing GPT-4’s reasoning​\n",
        "\n",
        "\n",
        "Fixes Applied:\n",
        "\n",
        "Swap answer order – judge both directions, count as tie if decision flips\n",
        "\n",
        "Chain-of-thought prompting – GPT-4 solves the problem before judging\n",
        "\n",
        "Reference-guided grading – use GPT-4's own answer as reference\n",
        "\n",
        "Few-shot examples – Add judgment examples to improve consistency (↑ to 77.5%)​\n",
        "\n",
        "\n",
        "![Alternate text ](Figure.png \"Title of the figure, location is simply the directory of the notebook\")"
      ],
      "metadata": {
        "id": "fw0JvxEa_6yG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation\n",
        "\n",
        "In this section, you will provide the code and its explanation. You may have to create more cells after this. (To keep the Notebook clean, do not display debugging output or thousands of print statements from hundreds of epochs. Make sure it is readable for others by reviewing it yourself carefully.)"
      ],
      "metadata": {
        "id": "Lu8SPBr3_6vV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "d1xD0fhs-hce",
        "outputId": "69aa7a10-305b-40dc-f064-e88a90ec21f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Apr 16 00:41:22 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQS0u8x3Cwwk",
        "outputId": "3e4a0c42-1a7a-4985-f95a-967dfe86a3f6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /FastChat.zip -d /src"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "eF0jI63MCwtx",
        "outputId": "eb9ac1f3-b61d-4d1b-8261-8184f7e6f23c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /FastChat.zip\n",
            "   creating: /src/FastChat/\n",
            "   creating: /src/FastChat/.github/\n",
            "  inflating: /src/FastChat/.github/PULL_REQUEST_TEMPLATE.md  \n",
            "   creating: /src/FastChat/.github/workflows/\n",
            "  inflating: /src/FastChat/.github/workflows/python-package.yml  \n",
            "  inflating: /src/FastChat/.gitignore  \n",
            "  inflating: /src/FastChat/.pylintrc  \n",
            "   creating: /src/FastChat/assets/\n",
            "  inflating: /src/FastChat/assets/demo_narrow.gif  \n",
            "  inflating: /src/FastChat/assets/qa_browser.png  \n",
            "  inflating: /src/FastChat/assets/screenshot_cli.png  \n",
            "  inflating: /src/FastChat/assets/screenshot_gui.png  \n",
            "  inflating: /src/FastChat/assets/server_arch.png  \n",
            "  inflating: /src/FastChat/assets/vicuna_logo.jpeg  \n",
            "   creating: /src/FastChat/data/\n",
            "  inflating: /src/FastChat/data/dummy_conversation.json  \n",
            "   creating: /src/FastChat/docker/\n",
            "  inflating: /src/FastChat/docker/docker-compose.yml  \n",
            "  inflating: /src/FastChat/docker/Dockerfile  \n",
            "   creating: /src/FastChat/docs/\n",
            "  inflating: /src/FastChat/docs/arena.md  \n",
            "  inflating: /src/FastChat/docs/awq.md  \n",
            "   creating: /src/FastChat/docs/commands/\n",
            "  inflating: /src/FastChat/docs/commands/conv_release.md  \n",
            "  inflating: /src/FastChat/docs/commands/data_cleaning.md  \n",
            "  inflating: /src/FastChat/docs/commands/leaderboard.md  \n",
            "  inflating: /src/FastChat/docs/commands/local_cluster.md  \n",
            "  inflating: /src/FastChat/docs/commands/pypi.md  \n",
            "  inflating: /src/FastChat/docs/commands/webserver.md  \n",
            "  inflating: /src/FastChat/docs/dashinfer_integration.md  \n",
            "  inflating: /src/FastChat/docs/dataset_release.md  \n",
            "  inflating: /src/FastChat/docs/exllama_v2.md  \n",
            "  inflating: /src/FastChat/docs/gptq.md  \n",
            "  inflating: /src/FastChat/docs/langchain_integration.md  \n",
            "  inflating: /src/FastChat/docs/lightllm_integration.md  \n",
            "  inflating: /src/FastChat/docs/mlx_integration.md  \n",
            "  inflating: /src/FastChat/docs/model_support.md  \n",
            "  inflating: /src/FastChat/docs/openai_api.md  \n",
            "  inflating: /src/FastChat/docs/server_arch.md  \n",
            "  inflating: /src/FastChat/docs/third_party_ui.md  \n",
            "  inflating: /src/FastChat/docs/training.md  \n",
            "  inflating: /src/FastChat/docs/vicuna_weights_version.md  \n",
            "  inflating: /src/FastChat/docs/vllm_integration.md  \n",
            "  inflating: /src/FastChat/docs/xFasterTransformer.md  \n",
            "   creating: /src/FastChat/fastchat/\n",
            "  inflating: /src/FastChat/fastchat/constants.py  \n",
            "  inflating: /src/FastChat/fastchat/conversation.py  \n",
            "   creating: /src/FastChat/fastchat/data/\n",
            "  inflating: /src/FastChat/fastchat/data/clean_sharegpt.py  \n",
            "  inflating: /src/FastChat/fastchat/data/convert_alpaca.py  \n",
            "  inflating: /src/FastChat/fastchat/data/extract_gpt4_only.py  \n",
            "  inflating: /src/FastChat/fastchat/data/extract_single_round.py  \n",
            "  inflating: /src/FastChat/fastchat/data/filter_wrong_format.py  \n",
            "  inflating: /src/FastChat/fastchat/data/get_stats.py  \n",
            "  inflating: /src/FastChat/fastchat/data/hardcoded_questions.py  \n",
            "  inflating: /src/FastChat/fastchat/data/inspect_data.py  \n",
            "  inflating: /src/FastChat/fastchat/data/merge.py  \n",
            "  inflating: /src/FastChat/fastchat/data/optional_clean.py  \n",
            "  inflating: /src/FastChat/fastchat/data/optional_replace.py  \n",
            "  inflating: /src/FastChat/fastchat/data/prepare_all.py  \n",
            "  inflating: /src/FastChat/fastchat/data/pretty_json.py  \n",
            "  inflating: /src/FastChat/fastchat/data/sample.py  \n",
            "  inflating: /src/FastChat/fastchat/data/split_long_conversation.py  \n",
            "  inflating: /src/FastChat/fastchat/data/split_train_test.py  \n",
            "  inflating: /src/FastChat/fastchat/data/__init__.py  \n",
            "   creating: /src/FastChat/fastchat/llm_judge/\n",
            "  inflating: /src/FastChat/fastchat/llm_judge/clean_judgment.py  \n",
            "  inflating: /src/FastChat/fastchat/llm_judge/common.py  \n",
            "  inflating: /src/FastChat/fastchat/llm_judge/compute_agreement.py  \n",
            "   creating: /src/FastChat/fastchat/llm_judge/data/\n",
            "  inflating: /src/FastChat/fastchat/llm_judge/data/judge_prompts.jsonl  \n",
            "   creating: /src/FastChat/fastchat/llm_judge/data/mt_bench/\n",
            "   creating: /src/FastChat/fastchat/llm_judge/data/mt_bench/misc/\n",
            "  inflating: /src/FastChat/fastchat/llm_judge/data/mt_bench/misc/radar.png  \n",
            "  inflating: /src/FastChat/fastchat/llm_judge/data/mt_bench/question.jsonl  \n",
            "   creating: /src/FastChat/fastchat/llm_judge/data/mt_bench/reference_answer/\n",
            "  inflating: /src/FastChat/fastchat/llm_judge/data/mt_bench/reference_answer/gpt-4.jsonl  \n",
            "   creating: /src/FastChat/fastchat/llm_judge/data/vicuna_bench/\n",
            "  inflating: /src/FastChat/fastchat/llm_judge/data/vicuna_bench/question.jsonl  \n",
            "   creating: /src/FastChat/fastchat/llm_judge/data/vicuna_bench/reference_answer/\n",
            "  inflating: /src/FastChat/fastchat/llm_judge/data/vicuna_bench/reference_answer/gpt-4.jsonl  \n",
            "  inflating: /src/FastChat/fastchat/llm_judge/download_mt_bench_pregenerated.py  \n",
            "  inflating: /src/FastChat/fastchat/llm_judge/gen_api_answer.py  \n",
            "  inflating: /src/FastChat/fastchat/llm_judge/gen_judgment.py  \n",
            "  inflating: /src/FastChat/fastchat/llm_judge/gen_model_answer.py  \n",
            "  inflating: /src/FastChat/fastchat/llm_judge/qa_browser.py  \n",
            "  inflating: /src/FastChat/fastchat/llm_judge/README.md  \n",
            "  inflating: /src/FastChat/fastchat/llm_judge/show_result.py  \n",
            "   creating: /src/FastChat/fastchat/model/\n",
            "  inflating: /src/FastChat/fastchat/model/apply_delta.py  \n",
            "  inflating: /src/FastChat/fastchat/model/apply_lora.py  \n",
            "  inflating: /src/FastChat/fastchat/model/compression.py  \n",
            "  inflating: /src/FastChat/fastchat/model/convert_fp16.py  \n",
            "  inflating: /src/FastChat/fastchat/model/llama_condense_monkey_patch.py  \n",
            "  inflating: /src/FastChat/fastchat/model/make_delta.py  \n",
            "  inflating: /src/FastChat/fastchat/model/model_adapter.py  \n",
            "  inflating: /src/FastChat/fastchat/model/model_chatglm.py  \n",
            "  inflating: /src/FastChat/fastchat/model/model_cllm.py  \n",
            "  inflating: /src/FastChat/fastchat/model/model_codet5p.py  \n",
            "  inflating: /src/FastChat/fastchat/model/model_exllama.py  \n",
            "  inflating: /src/FastChat/fastchat/model/model_falcon.py  \n",
            "  inflating: /src/FastChat/fastchat/model/model_registry.py  \n",
            "  inflating: /src/FastChat/fastchat/model/model_xfastertransformer.py  \n",
            "  inflating: /src/FastChat/fastchat/model/model_yuan2.py  \n",
            "  inflating: /src/FastChat/fastchat/model/monkey_patch_non_inplace.py  \n",
            "  inflating: /src/FastChat/fastchat/model/rwkv_model.py  \n",
            "  inflating: /src/FastChat/fastchat/model/upload_hub.py  \n",
            "  inflating: /src/FastChat/fastchat/model/__init__.py  \n",
            "   creating: /src/FastChat/fastchat/modules/\n",
            "  inflating: /src/FastChat/fastchat/modules/awq.py  \n",
            "  inflating: /src/FastChat/fastchat/modules/exllama.py  \n",
            "  inflating: /src/FastChat/fastchat/modules/gptq.py  \n",
            "  inflating: /src/FastChat/fastchat/modules/xfastertransformer.py  \n",
            "  inflating: /src/FastChat/fastchat/modules/__init__.py  \n",
            "   creating: /src/FastChat/fastchat/protocol/\n",
            "  inflating: /src/FastChat/fastchat/protocol/api_protocol.py  \n",
            "  inflating: /src/FastChat/fastchat/protocol/openai_api_protocol.py  \n",
            "   creating: /src/FastChat/fastchat/serve/\n",
            "  inflating: /src/FastChat/fastchat/serve/api_provider.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/base_model_worker.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/call_monitor.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/cli.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/controller.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/dashinfer_worker.py  \n",
            "   creating: /src/FastChat/fastchat/serve/example_images/\n",
            "  inflating: /src/FastChat/fastchat/serve/example_images/distracted.jpg  \n",
            "  inflating: /src/FastChat/fastchat/serve/example_images/fridge.jpg  \n",
            "   creating: /src/FastChat/fastchat/serve/gateway/\n",
            "  inflating: /src/FastChat/fastchat/serve/gateway/nginx.conf  \n",
            "  inflating: /src/FastChat/fastchat/serve/gateway/README.md  \n",
            "  inflating: /src/FastChat/fastchat/serve/gradio_block_arena_anony.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/gradio_block_arena_named.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/gradio_block_arena_vision.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/gradio_block_arena_vision_anony.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/gradio_block_arena_vision_named.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/gradio_global_state.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/gradio_web_server.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/gradio_web_server_multi.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/huggingface_api.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/huggingface_api_worker.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/inference.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/launch_all_serve.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/lightllm_worker.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/mlx_worker.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/model_worker.py  \n",
            "   creating: /src/FastChat/fastchat/serve/monitor/\n",
            "  inflating: /src/FastChat/fastchat/serve/monitor/add_markdown_info.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/monitor/basic_stats.py  \n",
            "   creating: /src/FastChat/fastchat/serve/monitor/classify/\n",
            "  inflating: /src/FastChat/fastchat/serve/monitor/classify/category.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/monitor/classify/config.yaml  \n",
            "  inflating: /src/FastChat/fastchat/serve/monitor/classify/display_score.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/monitor/classify/label.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/monitor/classify/README.md  \n",
            "  inflating: /src/FastChat/fastchat/serve/monitor/classify/vision_config.yaml  \n",
            "  inflating: /src/FastChat/fastchat/serve/monitor/clean_battle_data.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/monitor/clean_chat_data.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/monitor/copilot_arena.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/monitor/criteria_labeling.py  \n",
            "   creating: /src/FastChat/fastchat/serve/monitor/dataset_release_scripts/\n",
            "   creating: /src/FastChat/fastchat/serve/monitor/dataset_release_scripts/arena_33k/\n",
            "  inflating: /src/FastChat/fastchat/serve/monitor/dataset_release_scripts/arena_33k/count_unique_users.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/monitor/dataset_release_scripts/arena_33k/filter_bad_conv.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/monitor/dataset_release_scripts/arena_33k/merge_field.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/monitor/dataset_release_scripts/arena_33k/sample.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/monitor/dataset_release_scripts/arena_33k/upload_hf_dataset.py  \n",
            "   creating: /src/FastChat/fastchat/serve/monitor/dataset_release_scripts/lmsys_chat_1m/\n",
            "  inflating: /src/FastChat/fastchat/serve/monitor/dataset_release_scripts/lmsys_chat_1m/approve_all.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/monitor/dataset_release_scripts/lmsys_chat_1m/compute_stats.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/monitor/dataset_release_scripts/lmsys_chat_1m/filter_bad_conv.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/monitor/dataset_release_scripts/lmsys_chat_1m/final_post_processing.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/monitor/dataset_release_scripts/lmsys_chat_1m/instructions.md  \n",
            "  inflating: /src/FastChat/fastchat/serve/monitor/dataset_release_scripts/lmsys_chat_1m/merge_oai_tag.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/monitor/dataset_release_scripts/lmsys_chat_1m/process_all.sh  \n",
            "  inflating: /src/FastChat/fastchat/serve/monitor/dataset_release_scripts/lmsys_chat_1m/sample.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/monitor/dataset_release_scripts/lmsys_chat_1m/upload_hf_dataset.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/monitor/deduplication.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/monitor/elo_analysis.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/monitor/inspect_conv.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/monitor/intersect_conv_file.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/monitor/leaderboard_csv_to_html.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/monitor/monitor.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/monitor/monitor_md.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/monitor/rating_systems.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/monitor/summarize_cluster.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/monitor/tag_openai_moderation.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/monitor/topic_clustering.py  \n",
            "   creating: /src/FastChat/fastchat/serve/monitor/vote_time_stats/\n",
            "  inflating: /src/FastChat/fastchat/serve/monitor/vote_time_stats/analyze_data.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/monitor/vote_time_stats/plot.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/monitor/vote_time_stats/README.md  \n",
            "  inflating: /src/FastChat/fastchat/serve/multi_model_worker.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/openai_api_server.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/register_worker.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/remote_logger.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/sglang_worker.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/shutdown_serve.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/test_message.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/test_throughput.py  \n",
            "   creating: /src/FastChat/fastchat/serve/vision/\n",
            "  inflating: /src/FastChat/fastchat/serve/vision/create_vqa_examples_dir.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/vision/create_vqa_examples_json.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/vision/image.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/vllm_worker.py  \n",
            "  inflating: /src/FastChat/fastchat/serve/__init__.py  \n",
            "   creating: /src/FastChat/fastchat/train/\n",
            "  inflating: /src/FastChat/fastchat/train/llama2_flash_attn_monkey_patch.py  \n",
            "  inflating: /src/FastChat/fastchat/train/llama_flash_attn_monkey_patch.py  \n",
            "  inflating: /src/FastChat/fastchat/train/llama_xformers_attn_monkey_patch.py  \n",
            "  inflating: /src/FastChat/fastchat/train/train.py  \n",
            "  inflating: /src/FastChat/fastchat/train/train_baichuan.py  \n",
            "  inflating: /src/FastChat/fastchat/train/train_flant5.py  \n",
            "  inflating: /src/FastChat/fastchat/train/train_lora.py  \n",
            "  inflating: /src/FastChat/fastchat/train/train_lora_t5.py  \n",
            "  inflating: /src/FastChat/fastchat/train/train_mem.py  \n",
            "  inflating: /src/FastChat/fastchat/train/train_with_template.py  \n",
            "  inflating: /src/FastChat/fastchat/train/train_xformers.py  \n",
            "  inflating: /src/FastChat/fastchat/train/train_yuan2.py  \n",
            "  inflating: /src/FastChat/fastchat/utils.py  \n",
            "  inflating: /src/FastChat/fastchat/__init__.py  \n",
            "  inflating: /src/FastChat/format.sh  \n",
            "  inflating: /src/FastChat/LICENSE   \n",
            "   creating: /src/FastChat/playground/\n",
            "   creating: /src/FastChat/playground/benchmark/\n",
            "  inflating: /src/FastChat/playground/benchmark/benchmark_api_provider.py  \n",
            "  inflating: /src/FastChat/playground/deepspeed_config_s2.json  \n",
            "  inflating: /src/FastChat/playground/deepspeed_config_s3.json  \n",
            "  inflating: /src/FastChat/playground/FastChat_API_GoogleColab.ipynb  \n",
            "   creating: /src/FastChat/playground/test_embedding/\n",
            "  inflating: /src/FastChat/playground/test_embedding/README.md  \n",
            "  inflating: /src/FastChat/playground/test_embedding/test_classification.py  \n",
            "  inflating: /src/FastChat/playground/test_embedding/test_semantic_search.py  \n",
            "  inflating: /src/FastChat/playground/test_embedding/test_sentence_similarity.py  \n",
            "  inflating: /src/FastChat/playground/__init__.py  \n",
            "  inflating: /src/FastChat/pyproject.toml  \n",
            "  inflating: /src/FastChat/README.md  \n",
            "   creating: /src/FastChat/scripts/\n",
            "  inflating: /src/FastChat/scripts/build-api.sh  \n",
            "  inflating: /src/FastChat/scripts/test_readme_train.sh  \n",
            "  inflating: /src/FastChat/scripts/train_lora.sh  \n",
            "  inflating: /src/FastChat/scripts/train_vicuna_13b.sh  \n",
            "  inflating: /src/FastChat/scripts/train_vicuna_7b.sh  \n",
            "  inflating: /src/FastChat/scripts/upload_pypi.sh  \n",
            "   creating: /src/FastChat/tests/\n",
            "  inflating: /src/FastChat/tests/killall_python.sh  \n",
            "  inflating: /src/FastChat/tests/launch_openai_api_test_server.py  \n",
            "  inflating: /src/FastChat/tests/load_test.py  \n",
            "  inflating: /src/FastChat/tests/README.md  \n",
            "  inflating: /src/FastChat/tests/test_cli.py  \n",
            "  inflating: /src/FastChat/tests/test_cli_inputs.txt  \n",
            "  inflating: /src/FastChat/tests/test_image_utils.py  \n",
            "  inflating: /src/FastChat/tests/test_openai_api.py  \n",
            "  inflating: /src/FastChat/tests/test_openai_langchain.py  \n",
            "  inflating: /src/FastChat/tests/test_openai_vision_api.py  \n",
            "  inflating: /src/FastChat/theme.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Go into FastChat and install\n",
        "%cd /src/FastChat\n",
        "!pip install -e \".[model_worker,llm_judge]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RPIBYasoCwq9",
        "outputId": "bc45258b-1c11-47eb-c0c6-9358d2cf01d8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/src/FastChat\n",
            "Obtaining file:///src/FastChat\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from fschat==0.2.36) (3.11.15)\n",
            "Collecting fastapi (from fschat==0.2.36)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from fschat==0.2.36) (0.28.1)\n",
            "Collecting markdown2[all] (from fschat==0.2.36)\n",
            "  Downloading markdown2-2.5.3-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting nh3 (from fschat==0.2.36)\n",
            "  Downloading nh3-0.2.21-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fschat==0.2.36) (2.0.2)\n",
            "Requirement already satisfied: prompt_toolkit>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from fschat==0.2.36) (3.0.50)\n",
            "Requirement already satisfied: pydantic<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from fschat==0.2.36) (2.11.3)\n",
            "Collecting pydantic-settings (from fschat==0.2.36)\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from fschat==0.2.36) (5.9.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from fschat==0.2.36) (2.32.3)\n",
            "Requirement already satisfied: rich>=10.0.0 in /usr/local/lib/python3.11/dist-packages (from fschat==0.2.36) (13.9.4)\n",
            "Collecting shortuuid (from fschat==0.2.36)\n",
            "  Downloading shortuuid-1.0.13-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting tiktoken (from fschat==0.2.36)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting uvicorn (from fschat==0.2.36)\n",
            "  Downloading uvicorn-0.34.1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: accelerate>=0.21 in /usr/local/lib/python3.11/dist-packages (from fschat==0.2.36) (1.5.2)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (from fschat==0.2.36) (0.14.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from fschat==0.2.36) (0.2.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from fschat==0.2.36) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.11/dist-packages (from fschat==0.2.36) (4.51.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from fschat==0.2.36) (5.29.4)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (from fschat==0.2.36) (1.72.0)\n",
            "Collecting anthropic (from fschat==0.2.36)\n",
            "  Downloading anthropic-0.49.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting openai (from fschat==0.2.36)\n",
            "  Downloading openai-0.28.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting ray (from fschat==0.2.36)\n",
            "  Downloading ray-2.44.1-cp311-cp311-manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.21->fschat==0.2.36) (24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.21->fschat==0.2.36) (6.0.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.21->fschat==0.2.36) (0.30.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.21->fschat==0.2.36) (0.5.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic->fschat==0.2.36) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic->fschat==0.2.36) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic->fschat==0.2.36) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic->fschat==0.2.36) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from anthropic->fschat==0.2.36) (4.13.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx->fschat==0.2.36) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->fschat==0.2.36) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->fschat==0.2.36) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->fschat==0.2.36) (0.14.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai->fschat==0.2.36) (4.67.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt_toolkit>=3.0.0->fschat==0.2.36) (0.2.13)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.0.0->fschat==0.2.36) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.0.0->fschat==0.2.36) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.0.0->fschat==0.2.36) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->fschat==0.2.36) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->fschat==0.2.36) (2.3.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.0.0->fschat==0.2.36) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.0.0->fschat==0.2.36) (2.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->fschat==0.2.36) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->fschat==0.2.36) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->fschat==0.2.36) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->fschat==0.2.36) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->fschat==0.2.36)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->fschat==0.2.36)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->fschat==0.2.36)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->fschat==0.2.36)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->fschat==0.2.36)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->fschat==0.2.36)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->fschat==0.2.36)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->fschat==0.2.36)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->fschat==0.2.36)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->fschat==0.2.36) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->fschat==0.2.36) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->fschat==0.2.36) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->fschat==0.2.36)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->fschat==0.2.36) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->fschat==0.2.36) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->fschat==0.2.36) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31.0->fschat==0.2.36) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31.0->fschat==0.2.36) (0.21.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->fschat==0.2.36) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->fschat==0.2.36) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->fschat==0.2.36) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->fschat==0.2.36) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->fschat==0.2.36) (6.4.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->fschat==0.2.36) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->fschat==0.2.36) (1.19.0)\n",
            "Collecting starlette<0.47.0,>=0.40.0 (from fastapi->fschat==0.2.36)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting wavedrom (from markdown2[all]->fschat==0.2.36)\n",
            "  Downloading wavedrom-2.0.3.post3.tar.gz (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.7/137.7 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting latex2mathml (from markdown2[all]->fschat==0.2.36)\n",
            "  Downloading latex2mathml-3.77.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings->fschat==0.2.36)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray->fschat==0.2.36) (8.1.8)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from ray->fschat==0.2.36) (4.23.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray->fschat==0.2.36) (1.1.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.0.0->fschat==0.2.36) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->fschat==0.2.36) (3.0.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray->fschat==0.2.36) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray->fschat==0.2.36) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray->fschat==0.2.36) (0.24.0)\n",
            "Collecting svgwrite (from wavedrom->markdown2[all]->fschat==0.2.36)\n",
            "  Downloading svgwrite-1.4.3-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from wavedrom->markdown2[all]->fschat==0.2.36) (1.17.0)\n",
            "Downloading anthropic-0.49.0-py3-none-any.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nh3-0.2.21-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (739 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.0/739.0 kB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Downloading ray-2.44.1-cp311-cp311-manylinux2014_x86_64.whl (68.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.1/68.1 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shortuuid-1.0.13-py3-none-any.whl (10 kB)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.1-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading latex2mathml-3.77.0-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.7/73.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading markdown2-2.5.3-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: fschat, wavedrom\n",
            "  Building editable for fschat (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fschat: filename=fschat-0.2.36-0.editable-py3-none-any.whl size=15058 sha256=542d03051390ed06ab3531dbccd8f5d7117e2e2dcf51d3d6be9e3b1912290130\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-qxektf3t/wheels/e6/2e/96/fd0f07fff7c1d9c81dfe38183949bfcf41a44c9c7a730fd659\n",
            "  Building wheel for wavedrom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wavedrom: filename=wavedrom-2.0.3.post3-py2.py3-none-any.whl size=30084 sha256=93905cf2532bab567d1caa013290b422cfa69353a477b86f408a0ebc5957a7f6\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/cf/3b/4dcf6b22fa41c5ece715fa5f4e05afd683e7b0ce0f2fcc7bb6\n",
            "Successfully built fschat wavedrom\n",
            "Installing collected packages: uvicorn, svgwrite, shortuuid, python-dotenv, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nh3, markdown2, latex2mathml, wavedrom, tiktoken, starlette, nvidia-cusparse-cu12, nvidia-cudnn-cu12, pydantic-settings, openai, nvidia-cusolver-cu12, fastapi, anthropic, ray, fschat\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.72.0\n",
            "    Uninstalling openai-1.72.0:\n",
            "      Successfully uninstalled openai-1.72.0\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed anthropic-0.49.0 fastapi-0.115.12 fschat-0.2.36 latex2mathml-3.77.0 markdown2-2.5.3 nh3-0.2.21 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 openai-0.28.1 pydantic-settings-2.8.1 python-dotenv-1.1.0 ray-2.44.1 shortuuid-1.0.13 starlette-0.46.2 svgwrite-1.4.3 tiktoken-0.9.0 uvicorn-0.34.1 wavedrom-2.0.3.post3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Download and Review Pre-Generated Model Answers and Judgments on MT-bench Questions\n",
        "\n",
        "!python3 /src/FastChat/fastchat/llm_judge/download_mt_bench_pregenerated.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cGPZPNEdCwoh",
        "outputId": "b330e07b-ca67-4e0b-fad6-19845659b11c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wget -q --show-progress -O data/mt_bench/model_answer/alpaca-13b.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/alpaca-13b.jsonl\n",
            "data/mt_bench/model 100%[===================>]  64.63K  --.-KB/s    in 0.02s   \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/baize-v2-13b.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/baize-v2-13b.jsonl\n",
            "data/mt_bench/model 100%[===================>] 171.91K  --.-KB/s    in 0.08s   \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/chatglm-6b.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/chatglm-6b.jsonl\n",
            "data/mt_bench/model 100%[===================>] 183.80K  --.-KB/s    in 0.07s   \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/claude-instant-v1.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/claude-instant-v1.jsonl\n",
            "data/mt_bench/model 100%[===================>] 161.20K  --.-KB/s    in 0.07s   \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/claude-v1.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/claude-v1.jsonl\n",
            "data/mt_bench/model 100%[===================>] 170.59K  --.-KB/s    in 0.07s   \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/dolly-v2-12b.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/dolly-v2-12b.jsonl\n",
            "data/mt_bench/model 100%[===================>]  88.37K  --.-KB/s    in 0.01s   \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/falcon-40b-instruct.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/falcon-40b-instruct.jsonl\n",
            "data/mt_bench/model 100%[===================>] 101.44K  --.-KB/s    in 0.02s   \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/fastchat-t5-3b.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/fastchat-t5-3b.jsonl\n",
            "data/mt_bench/model 100%[===================>] 175.38K  --.-KB/s    in 0.07s   \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/gpt-3.5-turbo.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/gpt-3.5-turbo.jsonl\n",
            "data/mt_bench/model 100%[===================>] 154.56K  --.-KB/s    in 0.07s   \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/gpt-4.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/gpt-4.jsonl\n",
            "data/mt_bench/model 100%[===================>] 194.38K  --.-KB/s    in 0.07s   \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/gpt4all-13b-snoozy.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/gpt4all-13b-snoozy.jsonl\n",
            "data/mt_bench/model 100%[===================>] 139.48K  --.-KB/s    in 0.07s   \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/guanaco-33b.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/guanaco-33b.jsonl\n",
            "data/mt_bench/model 100%[===================>] 187.41K  --.-KB/s    in 0.02s   \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/guanaco-65b.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/guanaco-65b.jsonl\n",
            "data/mt_bench/model 100%[===================>] 177.82K  --.-KB/s    in 0.02s   \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/h2ogpt-oasst-open-llama-13b.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/h2ogpt-oasst-open-llama-13b.jsonl\n",
            "data/mt_bench/model 100%[===================>] 159.00K  --.-KB/s    in 0.02s   \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/koala-13b.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/koala-13b.jsonl\n",
            "data/mt_bench/model 100%[===================>] 187.34K  --.-KB/s    in 0.07s   \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/llama-13b.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/llama-13b.jsonl\n",
            "data/mt_bench/model 100%[===================>] 126.98K  --.-KB/s    in 0.02s   \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/mpt-30b-chat.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/mpt-30b-chat.jsonl\n",
            "data/mt_bench/model 100%[===================>] 157.30K  --.-KB/s    in 0.07s   \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/mpt-30b-instruct.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/mpt-30b-instruct.jsonl\n",
            "data/mt_bench/model 100%[===================>]  91.42K  --.-KB/s    in 0.02s   \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/mpt-7b-chat.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/mpt-7b-chat.jsonl\n",
            "data/mt_bench/model 100%[===================>] 130.53K  --.-KB/s    in 0.02s   \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/nous-hermes-13b.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/nous-hermes-13b.jsonl\n",
            "data/mt_bench/model 100%[===================>] 115.20K  --.-KB/s    in 0.01s   \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/oasst-sft-4-pythia-12b.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/oasst-sft-4-pythia-12b.jsonl\n",
            "data/mt_bench/model 100%[===================>] 117.77K  --.-KB/s    in 0.07s   \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/oasst-sft-7-llama-30b.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/oasst-sft-7-llama-30b.jsonl\n",
            "data/mt_bench/model 100%[===================>] 135.10K  --.-KB/s    in 0.07s   \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/palm-2-chat-bison-001.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/palm-2-chat-bison-001.jsonl\n",
            "data/mt_bench/model 100%[===================>] 117.59K  --.-KB/s    in 0.02s   \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/rwkv-4-raven-14b.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/rwkv-4-raven-14b.jsonl\n",
            "data/mt_bench/model 100%[===================>] 181.97K  --.-KB/s    in 0.08s   \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/stablelm-tuned-alpha-7b.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/stablelm-tuned-alpha-7b.jsonl\n",
            "data/mt_bench/model 100%[===================>] 386.35K  --.-KB/s    in 0.07s   \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/tulu-30b.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/tulu-30b.jsonl\n",
            "data/mt_bench/model 100%[===================>] 128.37K  --.-KB/s    in 0.02s   \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/vicuna-13b-v1.3.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/vicuna-13b-v1.3.jsonl\n",
            "data/mt_bench/model 100%[===================>] 182.69K  --.-KB/s    in 0.08s   \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/vicuna-33b-v1.3.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/vicuna-33b-v1.3.jsonl\n",
            "data/mt_bench/model 100%[===================>] 227.01K  --.-KB/s    in 0.1s    \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/vicuna-7b-v1.3.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/vicuna-7b-v1.3.jsonl\n",
            "data/mt_bench/model 100%[===================>] 180.99K  --.-KB/s    in 0.07s   \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/wizardlm-13b.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/wizardlm-13b.jsonl\n",
            "data/mt_bench/model 100%[===================>] 151.79K  --.-KB/s    in 0.07s   \n",
            "wget -q --show-progress -O data/mt_bench/model_answer/wizardlm-30b.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_answer/wizardlm-30b.jsonl\n",
            "data/mt_bench/model 100%[===================>] 157.63K  --.-KB/s    in 0.02s   \n",
            "wget -q --show-progress -O data/mt_bench/model_judgment/gpt-4_single.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_judgment/gpt-4_single.jsonl\n",
            "data/mt_bench/model 100%[===================>]  19.18M  36.7MB/s    in 0.5s    \n",
            "wget -q --show-progress -O data/mt_bench/model_judgment/gpt-4_pair.jsonl https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_judgment/gpt-4_pair.jsonl\n",
            "data/mt_bench/model 100%[===================>]  45.82M  47.0MB/s    in 1.0s    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate model answers to MT-bench questions\n",
        "\n",
        "!python /src/FastChat/fastchat/llm_judge/gen_model_answer.py --model-path lmsys/vicuna-7b-v1.5 --model-id vicuna-7b-v1.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "X9hnl33CCwle",
        "outputId": "45e8bb1f-4409-47ad-a40e-4ec2a7ade2b7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-16 01:25:28.911350: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744766728.946109   16607 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744766728.956828   16607 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-16 01:25:28.991145: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Output to data/mt_bench/model_answer/vicuna-7b-v1.5.jsonl\n",
            "tokenizer_config.json: 100% 749/749 [00:00<00:00, 6.59MB/s]\n",
            "tokenizer.model: 100% 500k/500k [00:00<00:00, 12.2MB/s]\n",
            "special_tokens_map.json: 100% 438/438 [00:00<00:00, 4.32MB/s]\n",
            "config.json: 100% 615/615 [00:00<00:00, 5.35MB/s]\n",
            "pytorch_model.bin.index.json: 100% 26.8k/26.8k [00:00<00:00, 30.9MB/s]\n",
            "Fetching 2 files:   0% 0/2 [00:00<?, ?it/s]\n",
            "pytorch_model-00001-of-00002.bin:   0% 0.00/9.98G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:   0% 0.00/3.50G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   0% 10.5M/9.98G [00:00<01:41, 98.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:   0% 10.5M/3.50G [00:00<00:36, 96.0MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   0% 31.5M/9.98G [00:00<01:15, 132MB/s] \u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:   1% 31.5M/3.50G [00:00<00:24, 142MB/s] \u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   1% 62.9M/9.98G [00:00<00:54, 181MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:   2% 62.9M/3.50G [00:00<00:18, 189MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model.safetensors.index.json: 100% 28.1k/28.1k [00:00<00:00, 44.5MB/s]\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   1% 83.9M/9.98G [00:00<00:55, 178MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:   2% 83.9M/3.50G [00:00<00:19, 175MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   1% 105M/9.98G [00:00<01:01, 160MB/s] \u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:   3% 105M/3.50G [00:00<00:20, 164MB/s] \u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   1% 126M/9.98G [00:00<01:02, 156MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:   4% 126M/3.50G [00:00<00:21, 159MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   1% 147M/9.98G [00:02<06:02, 27.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:   4% 147M/3.50G [00:02<02:03, 27.2MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   2% 168M/9.98G [00:03<04:26, 36.9MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:   5% 168M/3.50G [00:03<01:31, 36.6MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   2% 189M/9.98G [00:03<03:22, 48.4MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:   5% 189M/3.50G [00:03<01:08, 48.6MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   2% 210M/9.98G [00:03<02:48, 58.0MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:   6% 210M/3.50G [00:03<00:58, 55.8MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   2% 231M/9.98G [00:03<03:05, 52.6MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:   7% 231M/3.50G [00:03<01:03, 51.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   3% 252M/9.98G [00:04<02:29, 64.9MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:   7% 252M/3.50G [00:04<00:51, 63.0MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   3% 273M/9.98G [00:04<01:59, 81.5MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:   8% 273M/3.50G [00:04<00:40, 79.5MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   3% 294M/9.98G [00:04<01:51, 86.5MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:   8% 294M/3.50G [00:04<00:38, 82.3MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   3% 315M/9.98G [00:04<01:34, 102MB/s] \u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:   9% 315M/3.50G [00:04<00:32, 97.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   3% 336M/9.98G [00:04<01:21, 118MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   4% 357M/9.98G [00:04<01:10, 136MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  10% 336M/3.50G [00:04<00:29, 107MB/s] \u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   4% 377M/9.98G [00:04<01:07, 142MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  10% 357M/3.50G [00:04<00:26, 120MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   4% 398M/9.98G [00:04<01:03, 151MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  11% 377M/3.50G [00:04<00:23, 133MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   4% 419M/9.98G [00:05<00:59, 159MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  11% 398M/3.50G [00:05<00:21, 147MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   4% 440M/9.98G [00:05<00:57, 166MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  12% 419M/3.50G [00:05<00:19, 159MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  13% 440M/3.50G [00:05<00:21, 143MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   5% 461M/9.98G [00:05<01:08, 140MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   5% 482M/9.98G [00:05<01:04, 148MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  13% 461M/3.50G [00:05<00:20, 148MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   5% 503M/9.98G [00:05<01:01, 154MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  14% 482M/3.50G [00:05<00:21, 140MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   5% 524M/9.98G [00:05<01:06, 142MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  14% 503M/3.50G [00:05<00:22, 136MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   5% 545M/9.98G [00:05<01:07, 139MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  15% 524M/3.50G [00:05<00:22, 134MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   6% 566M/9.98G [00:06<01:15, 125MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  16% 545M/3.50G [00:06<00:23, 124MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   6% 587M/9.98G [00:06<01:09, 136MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  16% 566M/3.50G [00:06<00:22, 129MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   6% 608M/9.98G [00:06<01:14, 127MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  17% 587M/3.50G [00:06<00:22, 128MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   6% 629M/9.98G [00:06<01:13, 127MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  17% 608M/3.50G [00:06<00:22, 128MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  18% 629M/3.50G [00:06<00:21, 131MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   7% 650M/9.98G [00:06<01:25, 109MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  19% 650M/3.50G [00:06<00:23, 122MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   7% 671M/9.98G [00:07<01:22, 112MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  19% 671M/3.50G [00:07<00:25, 112MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   7% 692M/9.98G [00:07<01:24, 110MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  20% 692M/3.50G [00:07<00:23, 118MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   7% 713M/9.98G [00:07<01:25, 108MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  20% 713M/3.50G [00:07<00:23, 118MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   7% 734M/9.98G [00:07<01:24, 109MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  21% 734M/3.50G [00:07<00:23, 115MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   8% 755M/9.98G [00:07<01:23, 110MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  22% 755M/3.50G [00:07<00:24, 114MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   8% 776M/9.98G [00:07<01:19, 116MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  22% 776M/3.50G [00:08<00:26, 104MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   8% 797M/9.98G [00:08<01:30, 101MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  23% 797M/3.50G [00:08<00:25, 105MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   8% 818M/9.98G [00:08<01:24, 108MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  23% 818M/3.50G [00:08<00:24, 111MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   8% 839M/9.98G [00:08<01:19, 116MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  24% 839M/3.50G [00:08<00:24, 110MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   9% 860M/9.98G [00:08<01:33, 97.8MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  25% 860M/3.50G [00:09<00:27, 95.1MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   9% 881M/9.98G [00:09<01:31, 99.9MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  25% 881M/3.50G [00:09<00:26, 100MB/s] \u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   9% 912M/9.98G [00:09<01:16, 119MB/s] \u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  26% 902M/3.50G [00:09<00:23, 109MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   9% 933M/9.98G [00:09<01:17, 117MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  26% 923M/3.50G [00:09<00:24, 107MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  10% 954M/9.98G [00:09<01:16, 117MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  27% 944M/3.50G [00:09<00:22, 112MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  10% 975M/9.98G [00:09<01:18, 114MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  28% 965M/3.50G [00:09<00:21, 117MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  10% 996M/9.98G [00:09<01:09, 129MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  10% 1.02G/9.98G [00:10<01:03, 142MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  28% 986M/3.50G [00:09<00:20, 125MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  10% 1.04G/9.98G [00:10<01:00, 149MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  29% 1.01G/3.50G [00:10<00:19, 128MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  11% 1.06G/9.98G [00:10<00:56, 157MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  11% 1.09G/9.98G [00:10<00:49, 180MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  11% 1.12G/9.98G [00:10<00:43, 204MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  29% 1.03G/3.50G [00:10<00:25, 96.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  30% 1.05G/3.50G [00:10<00:22, 110MB/s] \u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  12% 1.15G/9.98G [00:10<00:45, 193MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  31% 1.07G/3.50G [00:10<00:19, 128MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  12% 1.18G/9.98G [00:10<00:48, 180MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  31% 1.09G/3.50G [00:10<00:18, 129MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  12% 1.21G/9.98G [00:10<00:47, 184MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  32% 1.11G/3.50G [00:10<00:17, 140MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  12% 1.23G/9.98G [00:11<00:51, 170MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  32% 1.13G/3.50G [00:11<00:17, 138MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.25G/9.98G [00:11<00:52, 165MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  33% 1.15G/3.50G [00:11<00:15, 149MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.27G/9.98G [00:11<00:56, 154MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  34% 1.17G/3.50G [00:11<00:16, 144MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.29G/9.98G [00:11<00:55, 157MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  34% 1.20G/3.50G [00:11<00:16, 142MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.31G/9.98G [00:11<01:02, 140MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  35% 1.22G/3.50G [00:11<00:16, 136MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.33G/9.98G [00:11<01:06, 130MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  35% 1.24G/3.50G [00:11<00:17, 130MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  36% 1.26G/3.50G [00:12<00:16, 132MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  14% 1.35G/9.98G [00:12<01:14, 116MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  37% 1.28G/3.50G [00:12<00:16, 132MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  14% 1.37G/9.98G [00:12<01:08, 126MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  37% 1.30G/3.50G [00:12<00:16, 130MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  14% 1.39G/9.98G [00:12<01:05, 132MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  38% 1.32G/3.50G [00:12<00:16, 130MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  14% 1.42G/9.98G [00:12<01:07, 127MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  14% 1.44G/9.98G [00:12<01:05, 131MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  38% 1.34G/3.50G [00:12<00:17, 125MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.46G/9.98G [00:12<01:04, 131MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  39% 1.36G/3.50G [00:12<00:17, 121MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.48G/9.98G [00:14<04:07, 34.3MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  40% 1.38G/3.50G [00:15<01:18, 27.0MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.49G/9.98G [00:15<04:43, 29.9MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  40% 1.42G/3.50G [00:15<00:50, 41.2MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.52G/9.98G [00:15<02:57, 47.7MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  41% 1.44G/3.50G [00:15<00:39, 52.6MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.54G/9.98G [00:15<02:18, 60.9MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  42% 1.46G/3.50G [00:15<00:31, 65.5MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  16% 1.56G/9.98G [00:15<01:57, 71.7MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  42% 1.48G/3.50G [00:15<00:25, 79.6MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  16% 1.58G/9.98G [00:15<01:38, 85.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  43% 1.50G/3.50G [00:15<00:22, 89.2MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  16% 1.60G/9.98G [00:15<01:28, 94.5MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  43% 1.52G/3.50G [00:15<00:19, 103MB/s] \u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  16% 1.63G/9.98G [00:16<01:15, 110MB/s] \u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  44% 1.54G/3.50G [00:16<00:16, 117MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  17% 1.65G/9.98G [00:16<01:06, 126MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  45% 1.56G/3.50G [00:16<00:15, 124MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  17% 1.67G/9.98G [00:16<01:01, 135MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  45% 1.58G/3.50G [00:16<00:14, 130MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  17% 1.69G/9.98G [00:16<00:58, 141MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  46% 1.61G/3.50G [00:16<00:12, 152MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  17% 1.71G/9.98G [00:16<00:57, 143MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  47% 1.64G/3.50G [00:16<00:11, 161MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  17% 1.73G/9.98G [00:16<00:56, 147MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  47% 1.66G/3.50G [00:16<00:12, 143MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.75G/9.98G [00:16<00:57, 144MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  48% 1.68G/3.50G [00:16<00:12, 141MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.77G/9.98G [00:16<00:58, 140MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  49% 1.70G/3.50G [00:17<00:12, 140MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.79G/9.98G [00:17<01:04, 127MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  49% 1.72G/3.50G [00:17<00:12, 140MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.81G/9.98G [00:17<00:57, 141MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  50% 1.74G/3.50G [00:17<00:14, 125MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.84G/9.98G [00:17<01:09, 118MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  50% 1.76G/3.50G [00:17<00:12, 140MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  19% 1.86G/9.98G [00:17<01:08, 118MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  51% 1.78G/3.50G [00:17<00:14, 121MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  19% 1.88G/9.98G [00:17<01:07, 120MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  52% 1.80G/3.50G [00:17<00:13, 121MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  19% 1.90G/9.98G [00:17<01:00, 133MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  52% 1.82G/3.50G [00:18<00:13, 126MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  19% 1.92G/9.98G [00:18<01:00, 134MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  53% 1.85G/3.50G [00:18<00:14, 112MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  19% 1.94G/9.98G [00:18<01:07, 120MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  53% 1.87G/3.50G [00:18<00:13, 124MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  20% 1.96G/9.98G [00:18<01:01, 130MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  20% 1.98G/9.98G [00:18<00:56, 141MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  54% 1.89G/3.50G [00:18<00:12, 129MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  20% 2.00G/9.98G [00:18<01:03, 126MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  55% 1.91G/3.50G [00:18<00:13, 115MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  20% 2.02G/9.98G [00:18<00:58, 136MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  55% 1.93G/3.50G [00:18<00:12, 124MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  20% 2.04G/9.98G [00:19<00:57, 139MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.07G/9.98G [00:19<01:02, 126MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  56% 1.95G/3.50G [00:19<00:15, 98.3MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.09G/9.98G [00:19<01:00, 130MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  56% 1.97G/3.50G [00:19<00:14, 108MB/s] \u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  57% 1.99G/3.50G [00:19<00:11, 127MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.12G/9.98G [00:19<00:55, 140MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  58% 2.01G/3.50G [00:21<00:53, 28.0MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.14G/9.98G [00:21<04:15, 30.7MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  58% 2.03G/3.50G [00:21<00:39, 36.7MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.16G/9.98G [00:21<03:17, 39.5MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  59% 2.06G/3.50G [00:21<00:29, 48.2MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.18G/9.98G [00:21<02:33, 50.9MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  59% 2.08G/3.50G [00:22<00:23, 61.1MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.20G/9.98G [00:22<02:03, 63.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  60% 2.10G/3.50G [00:22<00:18, 74.1MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.22G/9.98G [00:22<01:41, 76.8MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  61% 2.12G/3.50G [00:22<00:15, 88.2MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.24G/9.98G [00:25<06:55, 18.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.28G/9.98G [00:26<06:25, 20.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.29G/9.98G [00:26<05:40, 22.6MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  61% 2.14G/3.50G [00:27<01:43, 13.2MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.31G/9.98G [00:27<04:11, 30.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.33G/9.98G [00:27<03:10, 40.2MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  62% 2.16G/3.50G [00:27<01:14, 17.9MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.35G/9.98G [00:27<02:26, 52.2MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  62% 2.18G/3.50G [00:27<00:54, 24.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.37G/9.98G [00:27<01:57, 64.7MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  63% 2.20G/3.50G [00:27<00:39, 32.8MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.39G/9.98G [00:27<01:35, 79.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  64% 2.22G/3.50G [00:27<00:29, 42.9MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.41G/9.98G [00:27<01:20, 93.5MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  64% 2.24G/3.50G [00:27<00:22, 54.7MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.43G/9.98G [00:27<01:07, 111MB/s] \u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  65% 2.28G/3.50G [00:27<00:15, 77.3MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.45G/9.98G [00:27<00:58, 129MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  66% 2.30G/3.50G [00:28<00:13, 89.5MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.47G/9.98G [00:28<00:54, 137MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  66% 2.32G/3.50G [00:28<00:11, 101MB/s] \u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.50G/9.98G [00:28<00:51, 145MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  67% 2.34G/3.50G [00:28<00:11, 102MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.53G/9.98G [00:28<00:52, 143MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  67% 2.36G/3.50G [00:28<00:10, 106MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.55G/9.98G [00:28<00:55, 133MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  68% 2.38G/3.50G [00:28<00:09, 117MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.57G/9.98G [00:28<00:53, 140MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  69% 2.40G/3.50G [00:28<00:09, 115MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.59G/9.98G [00:28<00:54, 136MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  69% 2.42G/3.50G [00:29<00:09, 119MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.61G/9.98G [00:29<00:59, 123MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  70% 2.44G/3.50G [00:29<00:08, 127MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.63G/9.98G [00:29<00:57, 128MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  70% 2.46G/3.50G [00:29<00:08, 123MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.65G/9.98G [00:29<00:58, 125MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  71% 2.49G/3.50G [00:29<00:08, 120MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.67G/9.98G [00:29<00:56, 128MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  72% 2.51G/3.50G [00:29<00:07, 127MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.69G/9.98G [00:29<00:54, 134MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.72G/9.98G [00:29<00:53, 135MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  72% 2.53G/3.50G [00:29<00:07, 123MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  73% 2.55G/3.50G [00:30<00:07, 132MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.74G/9.98G [00:30<00:52, 137MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  73% 2.57G/3.50G [00:30<00:07, 123MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.76G/9.98G [00:30<01:00, 120MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  74% 2.59G/3.50G [00:30<00:06, 135MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.78G/9.98G [00:30<01:00, 119MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  75% 2.61G/3.50G [00:30<00:07, 121MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.80G/9.98G [00:30<00:56, 128MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  75% 2.63G/3.50G [00:30<00:06, 127MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.82G/9.98G [00:30<00:55, 129MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  76% 2.65G/3.50G [00:30<00:06, 129MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.84G/9.98G [00:31<01:07, 106MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  76% 2.67G/3.50G [00:31<00:06, 125MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  77% 2.69G/3.50G [00:31<00:05, 142MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.86G/9.98G [00:31<01:01, 117MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.88G/9.98G [00:31<00:53, 132MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  78% 2.72G/3.50G [00:31<00:06, 125MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.90G/9.98G [00:31<01:06, 107MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  78% 2.74G/3.50G [00:31<00:07, 104MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.93G/9.98G [00:31<01:06, 106MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  79% 2.76G/3.50G [00:31<00:06, 114MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  30% 2.95G/9.98G [00:31<01:05, 108MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  79% 2.78G/3.50G [00:32<00:06, 111MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  80% 2.80G/3.50G [00:32<00:06, 114MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  81% 2.82G/3.50G [00:32<00:05, 126MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  30% 2.97G/9.98G [00:32<01:22, 84.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  30% 2.98G/9.98G [00:32<01:20, 87.0MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  81% 2.84G/3.50G [00:32<00:05, 124MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  30% 3.00G/9.98G [00:32<01:13, 95.3MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  82% 2.86G/3.50G [00:32<00:05, 126MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  30% 3.01G/9.98G [00:32<01:12, 95.5MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  82% 2.88G/3.50G [00:32<00:04, 128MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  30% 3.03G/9.98G [00:32<01:05, 106MB/s] \u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  83% 2.90G/3.50G [00:32<00:04, 126MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.05G/9.98G [00:33<01:00, 115MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  84% 2.93G/3.50G [00:33<00:04, 129MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.07G/9.98G [00:33<00:54, 126MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  84% 2.95G/3.50G [00:33<00:04, 128MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.09G/9.98G [00:33<00:57, 119MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  85% 2.97G/3.50G [00:33<00:04, 126MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.11G/9.98G [00:33<00:55, 124MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  85% 2.99G/3.50G [00:33<00:03, 132MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.14G/9.98G [00:33<00:51, 133MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  86% 3.01G/3.50G [00:33<00:03, 139MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.16G/9.98G [00:33<00:51, 133MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  87% 3.03G/3.50G [00:33<00:03, 137MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.18G/9.98G [00:34<00:53, 127MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  87% 3.05G/3.50G [00:34<00:03, 124MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.20G/9.98G [00:34<00:51, 131MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  88% 3.07G/3.50G [00:34<00:03, 127MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.22G/9.98G [00:34<00:53, 127MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  88% 3.09G/3.50G [00:34<00:03, 130MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.25G/9.98G [00:34<00:43, 156MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  89% 3.11G/3.50G [00:34<00:02, 134MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.27G/9.98G [00:34<00:49, 134MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  90% 3.14G/3.50G [00:34<00:02, 127MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.29G/9.98G [00:34<00:50, 133MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  90% 3.16G/3.50G [00:34<00:02, 124MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.31G/9.98G [00:34<00:48, 137MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  91% 3.18G/3.50G [00:35<00:02, 123MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.33G/9.98G [00:35<00:49, 135MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  91% 3.20G/3.50G [00:35<00:02, 126MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.36G/9.98G [00:35<00:47, 138MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.38G/9.98G [00:35<00:43, 150MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  92% 3.22G/3.50G [00:35<00:02, 127MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.40G/9.98G [00:35<00:46, 140MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  93% 3.24G/3.50G [00:35<00:01, 131MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.42G/9.98G [00:35<00:43, 151MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  93% 3.26G/3.50G [00:35<00:01, 137MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.44G/9.98G [00:35<00:41, 159MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  94% 3.28G/3.50G [00:35<00:01, 149MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  35% 3.46G/9.98G [00:35<00:45, 143MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  94% 3.30G/3.50G [00:35<00:01, 139MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  35% 3.48G/9.98G [00:36<00:45, 141MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  95% 3.32G/3.50G [00:36<00:01, 130MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  35% 3.50G/9.98G [00:36<00:47, 137MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  96% 3.34G/3.50G [00:36<00:01, 118MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  35% 3.52G/9.98G [00:36<00:46, 139MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  96% 3.37G/3.50G [00:36<00:01, 124MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.54G/9.98G [00:36<00:50, 127MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  97% 3.39G/3.50G [00:36<00:00, 124MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.57G/9.98G [00:36<00:48, 133MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  97% 3.41G/3.50G [00:36<00:00, 118MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.59G/9.98G [00:36<00:52, 122MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  98% 3.43G/3.50G [00:37<00:00, 121MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.61G/9.98G [00:37<00:47, 134MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  99% 3.45G/3.50G [00:37<00:00, 125MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.63G/9.98G [00:37<00:52, 121MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  99% 3.47G/3.50G [00:37<00:00, 140MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  37% 3.65G/9.98G [00:37<00:50, 126MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin: 100% 3.49G/3.50G [00:37<00:00, 128MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin: 100% 3.50G/3.50G [00:38<00:00, 90.7MB/s]\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  37% 3.69G/9.98G [00:39<03:43, 28.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  37% 3.73G/9.98G [00:39<02:08, 48.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  38% 3.76G/9.98G [00:39<01:32, 67.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  38% 3.80G/9.98G [00:40<01:09, 88.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  38% 3.83G/9.98G [00:40<00:57, 107MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  39% 3.86G/9.98G [00:40<00:45, 134MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  39% 3.89G/9.98G [00:40<00:37, 162MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  39% 3.92G/9.98G [00:40<00:32, 187MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  40% 3.95G/9.98G [00:40<00:28, 209MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  40% 3.98G/9.98G [00:40<00:26, 223MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  40% 4.02G/9.98G [00:40<00:25, 230MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  41% 4.05G/9.98G [00:41<00:24, 245MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  41% 4.08G/9.98G [00:41<00:23, 249MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  41% 4.11G/9.98G [00:41<00:23, 251MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.14G/9.98G [00:41<00:22, 258MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.17G/9.98G [00:41<00:21, 268MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.20G/9.98G [00:41<00:20, 276MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.24G/9.98G [00:41<00:21, 272MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  43% 4.27G/9.98G [00:41<00:21, 263MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  43% 4.30G/9.98G [00:41<00:21, 267MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  43% 4.33G/9.98G [00:42<00:20, 276MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.36G/9.98G [00:42<00:20, 270MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.39G/9.98G [00:42<00:21, 259MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.42G/9.98G [00:42<00:20, 272MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  45% 4.46G/9.98G [00:42<00:20, 265MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  45% 4.49G/9.98G [00:42<00:20, 267MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  45% 4.52G/9.98G [00:42<00:20, 272MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.55G/9.98G [00:42<00:20, 268MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.58G/9.98G [00:42<00:19, 275MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.61G/9.98G [00:43<00:19, 279MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  47% 4.65G/9.98G [00:43<00:19, 273MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  47% 4.68G/9.98G [00:43<00:19, 272MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  47% 4.71G/9.98G [00:43<00:18, 282MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.74G/9.98G [00:43<00:18, 282MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.77G/9.98G [00:43<00:21, 239MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.80G/9.98G [00:43<00:20, 256MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.83G/9.98G [00:43<00:19, 268MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.87G/9.98G [00:44<00:18, 280MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.90G/9.98G [00:44<00:21, 234MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.93G/9.98G [00:44<00:24, 208MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  50% 4.96G/9.98G [00:44<00:26, 193MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  50% 4.98G/9.98G [00:44<00:26, 189MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  50% 5.00G/9.98G [00:48<03:34, 23.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  51% 5.04G/9.98G [00:48<02:12, 37.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  51% 5.09G/9.98G [00:48<01:28, 55.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  51% 5.12G/9.98G [00:48<01:08, 71.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  52% 5.15G/9.98G [00:48<00:54, 88.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  52% 5.18G/9.98G [00:48<00:43, 111MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  52% 5.21G/9.98G [00:48<00:35, 134MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.24G/9.98G [00:48<00:29, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.27G/9.98G [00:49<00:25, 181MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.31G/9.98G [00:49<00:23, 202MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.34G/9.98G [00:49<00:20, 225MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.37G/9.98G [00:49<00:19, 231MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.40G/9.98G [00:49<00:19, 238MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.43G/9.98G [00:49<00:17, 255MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  55% 5.47G/9.98G [00:49<00:17, 261MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  55% 5.51G/9.98G [00:49<00:17, 252MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  55% 5.54G/9.98G [00:50<00:19, 230MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  56% 5.57G/9.98G [00:50<00:20, 212MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  56% 5.60G/9.98G [00:51<01:17, 56.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  56% 5.63G/9.98G [00:51<00:59, 72.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.66G/9.98G [00:52<00:46, 92.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.69G/9.98G [00:52<00:37, 114MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.73G/9.98G [00:52<00:30, 140MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  58% 5.76G/9.98G [00:52<00:26, 162MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  58% 5.79G/9.98G [00:52<00:23, 181MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  58% 5.82G/9.98G [00:52<00:21, 197MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  59% 5.85G/9.98G [00:52<00:18, 218MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  59% 5.88G/9.98G [00:52<00:17, 230MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  59% 5.91G/9.98G [00:53<00:16, 240MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  60% 5.95G/9.98G [00:53<00:16, 251MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  60% 5.98G/9.98G [00:53<00:16, 245MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  60% 6.01G/9.98G [00:53<00:16, 247MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.04G/9.98G [00:53<00:15, 247MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.07G/9.98G [00:53<00:15, 258MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.10G/9.98G [00:53<00:15, 257MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.13G/9.98G [00:53<00:15, 253MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  62% 6.17G/9.98G [00:54<00:15, 248MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  62% 6.20G/9.98G [00:54<00:17, 213MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  62% 6.23G/9.98G [01:00<03:46, 16.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.26G/9.98G [01:00<02:40, 23.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.29G/9.98G [01:00<01:55, 32.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.33G/9.98G [01:00<01:16, 47.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  64% 6.36G/9.98G [01:00<01:00, 60.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  64% 6.40G/9.98G [01:00<00:46, 76.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  64% 6.43G/9.98G [01:00<00:36, 97.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  65% 6.46G/9.98G [01:01<00:29, 120MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  65% 6.49G/9.98G [01:01<00:23, 146MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  65% 6.52G/9.98G [01:01<00:20, 167MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  66% 6.55G/9.98G [01:01<00:18, 186MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  66% 6.60G/9.98G [01:01<00:15, 215MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  66% 6.63G/9.98G [01:01<00:15, 216MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  67% 6.66G/9.98G [01:01<00:15, 212MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  67% 6.69G/9.98G [01:01<00:14, 232MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  67% 6.72G/9.98G [01:02<00:14, 218MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  68% 6.75G/9.98G [01:02<00:18, 175MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  68% 6.77G/9.98G [01:04<01:21, 39.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  68% 6.79G/9.98G [01:04<01:05, 48.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  68% 6.83G/9.98G [01:04<00:46, 67.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  69% 6.86G/9.98G [01:04<00:35, 88.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  69% 6.89G/9.98G [01:04<00:28, 110MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  69% 6.92G/9.98G [01:05<00:22, 137MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  70% 6.95G/9.98G [01:05<00:19, 159MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  70% 6.98G/9.98G [01:05<00:16, 183MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  70% 7.01G/9.98G [01:05<00:14, 203MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.05G/9.98G [01:05<00:13, 215MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.08G/9.98G [01:05<00:12, 230MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.11G/9.98G [01:05<00:11, 243MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  72% 7.14G/9.98G [01:05<00:11, 256MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  72% 7.18G/9.98G [01:05<00:10, 275MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  72% 7.21G/9.98G [01:06<00:09, 279MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.25G/9.98G [01:06<00:09, 275MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.28G/9.98G [01:06<00:12, 211MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.31G/9.98G [01:06<00:12, 209MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.34G/9.98G [01:06<00:11, 231MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.37G/9.98G [01:07<00:18, 144MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.39G/9.98G [01:10<01:45, 24.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.42G/9.98G [01:10<01:13, 34.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  75% 7.47G/9.98G [01:10<00:47, 52.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  75% 7.50G/9.98G [01:10<00:36, 68.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  75% 7.53G/9.98G [01:10<00:28, 87.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  76% 7.56G/9.98G [01:11<00:27, 88.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  76% 7.59G/9.98G [01:11<00:21, 112MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  76% 7.62G/9.98G [01:11<00:17, 135MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.65G/9.98G [01:11<00:15, 154MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.69G/9.98G [01:11<00:12, 179MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.72G/9.98G [01:11<00:11, 198MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  78% 7.75G/9.98G [01:14<01:05, 33.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  78% 7.78G/9.98G [01:14<00:47, 46.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  78% 7.82G/9.98G [01:14<00:32, 67.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.85G/9.98G [01:14<00:25, 83.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.89G/9.98G [01:15<00:20, 103MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.92G/9.98G [01:15<00:16, 122MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  80% 7.95G/9.98G [01:15<00:14, 138MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  80% 7.98G/9.98G [01:15<00:12, 159MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  80% 8.01G/9.98G [01:15<00:10, 184MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.04G/9.98G [01:15<00:09, 204MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.07G/9.98G [01:15<00:08, 222MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.11G/9.98G [01:15<00:08, 227MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  82% 8.14G/9.98G [01:16<00:07, 240MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  82% 8.17G/9.98G [01:16<00:07, 250MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  82% 8.20G/9.98G [01:16<00:07, 248MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.23G/9.98G [01:16<00:06, 265MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.26G/9.98G [01:16<00:06, 265MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.29G/9.98G [01:16<00:06, 270MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.33G/9.98G [01:16<00:06, 244MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  84% 8.36G/9.98G [01:16<00:06, 250MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  84% 8.39G/9.98G [01:17<00:05, 265MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  84% 8.42G/9.98G [01:17<00:05, 269MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.45G/9.98G [01:17<00:05, 267MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.48G/9.98G [01:17<00:06, 236MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.51G/9.98G [01:17<00:06, 224MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  86% 8.55G/9.98G [01:17<00:06, 237MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  86% 8.58G/9.98G [01:17<00:05, 252MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  86% 8.61G/9.98G [01:17<00:05, 255MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  87% 8.64G/9.98G [01:18<00:05, 266MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  87% 8.67G/9.98G [01:18<00:05, 258MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  87% 8.70G/9.98G [01:18<00:05, 253MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.73G/9.98G [01:18<00:04, 259MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.77G/9.98G [01:18<00:04, 272MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.81G/9.98G [01:18<00:04, 285MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  89% 8.84G/9.98G [01:18<00:04, 272MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  89% 8.87G/9.98G [01:19<00:05, 203MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  89% 8.90G/9.98G [01:19<00:05, 193MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  90% 8.93G/9.98G [01:19<00:07, 133MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  90% 8.95G/9.98G [01:22<00:38, 26.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  90% 8.99G/9.98G [01:22<00:27, 36.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  90% 9.02G/9.98G [01:23<00:19, 49.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  91% 9.05G/9.98G [01:23<00:13, 66.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  91% 9.08G/9.98G [01:23<00:10, 85.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  91% 9.11G/9.98G [01:23<00:08, 107MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.14G/9.98G [01:23<00:06, 127MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.18G/9.98G [01:23<00:05, 151MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.21G/9.98G [01:23<00:04, 173MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.25G/9.98G [01:23<00:03, 206MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.28G/9.98G [01:24<00:03, 216MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.32G/9.98G [01:24<00:02, 241MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  94% 9.35G/9.98G [01:24<00:02, 234MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  94% 9.38G/9.98G [01:24<00:02, 230MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  94% 9.42G/9.98G [01:24<00:02, 187MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  95% 9.45G/9.98G [01:30<00:32, 16.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  95% 9.48G/9.98G [01:31<00:22, 22.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  95% 9.51G/9.98G [01:31<00:15, 30.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  96% 9.54G/9.98G [01:31<00:10, 41.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  96% 9.57G/9.98G [01:31<00:07, 54.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  96% 9.60G/9.98G [01:31<00:05, 69.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.64G/9.98G [01:31<00:03, 88.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.67G/9.98G [01:31<00:02, 107MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.70G/9.98G [01:32<00:02, 129MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.73G/9.98G [01:32<00:01, 151MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.76G/9.98G [01:32<00:01, 167MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.79G/9.98G [01:32<00:01, 183MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.83G/9.98G [01:32<00:00, 181MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  99% 9.86G/9.98G [01:33<00:01, 119MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  99% 9.88G/9.98G [01:33<00:00, 119MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  99% 9.90G/9.98G [01:37<00:03, 19.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin: 100% 9.94G/9.98G [01:37<00:01, 32.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin: 100% 9.98G/9.98G [01:37<00:00, 102MB/s] \n",
            "Fetching 2 files: 100% 2/2 [01:37<00:00, 48.76s/it]\n",
            "Loading checkpoint shards: 100% 2/2 [00:00<00:00,  4.23it/s]\n",
            "generation_config.json: 100% 162/162 [00:00<00:00, 1.65MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "  5% 4/80 [02:52<54:18, 42.88s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "100% 80/80 [47:08<00:00, 35.35s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ],
      "metadata": {
        "id": "XYdk6UGECwf9"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /src/FastChat/fastchat/llm_judge/gen_judgment.py --model-list vicuna-7b-v1.5 --parallel 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "eZMcmHNXCwdU",
        "outputId": "5a416207-e243-4353-a17a-c5163a8e6ee6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-16 02:45:13.929376: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744771513.952535   36739 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744771513.959121   36739 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-16 02:45:13.981893: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Stats:\n",
            "{\n",
            "    \"bench_name\": \"mt_bench\",\n",
            "    \"mode\": \"single\",\n",
            "    \"judge\": \"gpt-4\",\n",
            "    \"baseline\": null,\n",
            "    \"model_list\": [\n",
            "        \"vicuna-7b-v1.5\"\n",
            "    ],\n",
            "    \"total_num_questions\": 80,\n",
            "    \"total_num_matches\": 160,\n",
            "    \"output_path\": \"data/mt_bench/model_judgment/gpt-4_single.jsonl\"\n",
            "}\n",
            "Press Enter to confirm...\n",
            "  0% 0/160 [00:00<?, ?it/s]question: 141, turn: 2, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "  1% 1/160 [00:04<12:01,  4.54s/it]question: 143, turn: 2, model: vicuna-7b-v1.5, score: 3, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "  1% 2/160 [00:07<08:53,  3.38s/it]question: 88, turn: 1, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-v1')\n",
            "question: 155, turn: 1, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-v1')\n",
            "question: 132, turn: 2, model: vicuna-7b-v1.5, score: 8, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "question: 153, turn: 2, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "question: 114, turn: 2, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            "  2% 3/160 [00:23<24:56,  9.53s/it]question: 86, turn: 2, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "question: 87, turn: 2, model: vicuna-7b-v1.5, score: 8, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "question: 117, turn: 1, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-math-v1')\n",
            "  5% 8/160 [00:38<11:26,  4.51s/it]question: 104, turn: 2, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            "  7% 11/160 [00:41<07:30,  3.02s/it]question: 93, turn: 2, model: vicuna-7b-v1.5, score: 8, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "  8% 12/160 [00:43<07:13,  2.93s/it]question: 137, turn: 1, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1')\n",
            "  8% 13/160 [00:44<06:09,  2.51s/it]question: 150, turn: 2, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "question: 113, turn: 1, model: vicuna-7b-v1.5, score: 2, judge: ('gpt-4', 'single-math-v1')\n",
            "question: 117, turn: 2, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            "  9% 14/160 [00:58<11:58,  4.92s/it]question: 102, turn: 1, model: vicuna-7b-v1.5, score: 2, judge: ('gpt-4', 'single-math-v1')\n",
            " 11% 17/160 [01:01<07:24,  3.11s/it]question: 98, turn: 2, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            " 11% 18/160 [01:02<06:36,  2.79s/it]question: 159, turn: 2, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            " 12% 19/160 [01:06<06:54,  2.94s/it]question: 91, turn: 2, model: vicuna-7b-v1.5, score: 6, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            " 12% 20/160 [01:07<05:54,  2.53s/it]question: 156, turn: 1, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-v1')\n",
            " 13% 21/160 [01:09<05:32,  2.39s/it]question: 107, turn: 1, model: vicuna-7b-v1.5, score: 2, judge: ('gpt-4', 'single-math-v1')\n",
            " 14% 22/160 [01:11<05:04,  2.20s/it]question: 110, turn: 1, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-math-v1')\n",
            " 14% 23/160 [01:13<04:59,  2.19s/it]question: 89, turn: 1, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1')\n",
            " 15% 24/160 [01:14<04:32,  2.00s/it]question: 144, turn: 1, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1')\n",
            "question: 148, turn: 1, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1')\n",
            " 16% 25/160 [01:19<05:55,  2.63s/it]question: 112, turn: 1, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-math-v1')\n",
            "question: 99, turn: 2, model: vicuna-7b-v1.5, score: 3, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "question: 118, turn: 2, model: vicuna-7b-v1.5, score: 2, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            " 17% 27/160 [01:27<07:32,  3.40s/it]question: 152, turn: 2, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            " 19% 30/160 [01:31<05:03,  2.33s/it]question: 106, turn: 2, model: vicuna-7b-v1.5, score: 2, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            " 19% 31/160 [01:33<05:03,  2.35s/it]question: 135, turn: 1, model: vicuna-7b-v1.5, score: 8, judge: ('gpt-4', 'single-v1')\n",
            " 20% 32/160 [01:34<04:11,  1.97s/it]question: 81, turn: 2, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "question: 105, turn: 1, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-math-v1')\n",
            " 21% 33/160 [01:41<06:32,  3.09s/it]question: 100, turn: 1, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-v1')\n",
            " 22% 35/160 [01:44<05:20,  2.56s/it]question: 102, turn: 2, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            "question: 125, turn: 1, model: vicuna-7b-v1.5, score: 2, judge: ('gpt-4', 'single-math-v1')\n",
            " 22% 36/160 [01:50<06:53,  3.34s/it]question: 97, turn: 1, model: vicuna-7b-v1.5, score: 6, judge: ('gpt-4', 'single-v1')\n",
            " 24% 38/160 [01:53<05:17,  2.60s/it]question: 151, turn: 2, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "question: 151, turn: 1, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1')\n",
            "question: 111, turn: 1, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-math-v1')\n",
            " 24% 39/160 [02:05<09:22,  4.65s/it]question: 133, turn: 1, model: vicuna-7b-v1.5, score: 4, judge: ('gpt-4', 'single-v1')\n",
            " 26% 42/160 [02:06<04:51,  2.47s/it]question: 122, turn: 2, model: vicuna-7b-v1.5, score: 2, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            "question: 114, turn: 1, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-math-v1')\n",
            " 27% 43/160 [02:21<09:25,  4.83s/it]question: 142, turn: 2, model: vicuna-7b-v1.5, score: 7, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            " 28% 45/160 [02:23<06:56,  3.62s/it]question: 107, turn: 2, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            "question: 116, turn: 2, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            " 29% 46/160 [02:28<07:23,  3.89s/it]question: 149, turn: 2, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "question: 138, turn: 1, model: vicuna-7b-v1.5, score: 8, judge: ('gpt-4', 'single-v1')\n",
            " 30% 48/160 [02:35<06:56,  3.72s/it]question: 96, turn: 2, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            " 31% 50/160 [02:36<04:49,  2.63s/it]<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-m8ABNiJGMemgtsShkmm6M2va on tokens per min (TPM): Limit 10000, Used 9548, Requested 644. Please try again in 1.152s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 106, turn: 1, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-math-v1')\n",
            " 32% 51/160 [02:38<04:26,  2.44s/it]question: 99, turn: 1, model: vicuna-7b-v1.5, score: 3, judge: ('gpt-4', 'single-v1')\n",
            "<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-m8ABNiJGMemgtsShkmm6M2va on tokens per min (TPM): Limit 10000, Used 8849, Requested 1959. Please try again in 4.848s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 140, turn: 2, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            " 32% 52/160 [02:53<09:20,  5.19s/it]<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-m8ABNiJGMemgtsShkmm6M2va on tokens per min (TPM): Limit 10000, Used 8690, Requested 1959. Please try again in 3.894s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 141, turn: 1, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-v1')\n",
            "question: 95, turn: 2, model: vicuna-7b-v1.5, score: 5, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-m8ABNiJGMemgtsShkmm6M2va on tokens per min (TPM): Limit 10000, Used 8359, Requested 1959. Please try again in 1.908s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 124, turn: 1, model: vicuna-7b-v1.5, score: 2, judge: ('gpt-4', 'single-math-v1')\n",
            "question: 138, turn: 2, model: vicuna-7b-v1.5, score: 7, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-m8ABNiJGMemgtsShkmm6M2va on tokens per min (TPM): Limit 10000, Used 9419, Requested 742. Please try again in 966ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 128, turn: 2, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            " 34% 54/160 [03:28<17:29,  9.90s/it]<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-m8ABNiJGMemgtsShkmm6M2va on tokens per min (TPM): Limit 10000, Used 8598, Requested 1515. Please try again in 678ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 83, turn: 1, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1')\n",
            " 37% 59/160 [03:31<07:35,  4.51s/it]question: 84, turn: 2, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-m8ABNiJGMemgtsShkmm6M2va on tokens per min (TPM): Limit 10000, Used 9202, Requested 1515. Please try again in 4.302s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 154, turn: 1, model: vicuna-7b-v1.5, score: 8, judge: ('gpt-4', 'single-v1')\n",
            "question: 122, turn: 1, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-math-v1')\n",
            "<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-m8ABNiJGMemgtsShkmm6M2va on tokens per min (TPM): Limit 10000, Used 9863, Requested 504. Please try again in 2.202s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 91, turn: 1, model: vicuna-7b-v1.5, score: 8.5, judge: ('gpt-4', 'single-v1')\n",
            "question: 110, turn: 2, model: vicuna-7b-v1.5, score: 7, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            " 38% 60/160 [04:03<14:03,  8.44s/it]<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-m8ABNiJGMemgtsShkmm6M2va on tokens per min (TPM): Limit 10000, Used 8788, Requested 1560. Please try again in 2.088s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 112, turn: 2, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            " 41% 65/160 [04:09<07:40,  4.84s/it]question: 97, turn: 2, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "question: 90, turn: 2, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "question: 115, turn: 2, model: vicuna-7b-v1.5, score: 2, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            "<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-m8ABNiJGMemgtsShkmm6M2va on tokens per min (TPM): Limit 10000, Used 9211, Requested 1618. Please try again in 4.974s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 125, turn: 2, model: vicuna-7b-v1.5, score: 2, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            " 41% 66/160 [04:24<09:34,  6.11s/it]<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-m8ABNiJGMemgtsShkmm6M2va on tokens per min (TPM): Limit 10000, Used 9478, Requested 886. Please try again in 2.184s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-m8ABNiJGMemgtsShkmm6M2va on tokens per min (TPM): Limit 10000, Used 9415, Requested 886. Please try again in 1.806s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 155, turn: 2, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "question: 103, turn: 2, model: vicuna-7b-v1.5, score: 7, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            " 44% 70/160 [04:49<09:14,  6.16s/it]question: 101, turn: 1, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-math-v1')\n",
            " 45% 72/160 [04:51<07:18,  4.99s/it]question: 131, turn: 2, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            " 46% 73/160 [04:53<06:28,  4.46s/it]<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-m8ABNiJGMemgtsShkmm6M2va on tokens per min (TPM): Limit 10000, Used 9632, Requested 455. Please try again in 522ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 113, turn: 2, model: vicuna-7b-v1.5, score: 2, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            " 46% 74/160 [04:57<06:23,  4.46s/it]question: 137, turn: 2, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "question: 139, turn: 2, model: vicuna-7b-v1.5, score: 2, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "question: 120, turn: 1, model: vicuna-7b-v1.5, score: 2, judge: ('gpt-4', 'single-math-v1')\n",
            "question: 115, turn: 1, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-math-v1')\n",
            " 47% 75/160 [05:14<09:35,  6.77s/it]<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-m8ABNiJGMemgtsShkmm6M2va on tokens per min (TPM): Limit 10000, Used 9722, Requested 587. Please try again in 1.854s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 160, turn: 1, model: vicuna-7b-v1.5, score: 8.5, judge: ('gpt-4', 'single-v1')\n",
            " 49% 79/160 [05:15<04:34,  3.39s/it]<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-m8ABNiJGMemgtsShkmm6M2va on tokens per min (TPM): Limit 10000, Used 9566, Requested 851. Please try again in 2.502s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 159, turn: 1, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1')\n",
            " 50% 80/160 [05:27<06:27,  4.84s/it]<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-m8ABNiJGMemgtsShkmm6M2va on tokens per min (TPM): Limit 10000, Used 8858, Requested 1631. Please try again in 2.934s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 94, turn: 1, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1')\n",
            " 51% 81/160 [05:30<05:46,  4.39s/it]question: 134, turn: 1, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-v1')\n",
            "question: 144, turn: 2, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-m8ABNiJGMemgtsShkmm6M2va on tokens per min (TPM): Limit 10000, Used 8648, Requested 1631. Please try again in 1.674s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 131, turn: 1, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1')\n",
            "question: 96, turn: 1, model: vicuna-7b-v1.5, score: 8, judge: ('gpt-4', 'single-v1')\n",
            "<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-m8ABNiJGMemgtsShkmm6M2va on tokens per min (TPM): Limit 10000, Used 8459, Requested 1631. Please try again in 540ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 129, turn: 1, model: vicuna-7b-v1.5, score: 2, judge: ('gpt-4', 'single-math-v1')\n",
            "question: 129, turn: 2, model: vicuna-7b-v1.5, score: 2, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            " 51% 82/160 [06:10<15:16, 11.75s/it]question: 103, turn: 1, model: vicuna-7b-v1.5, score: 6, judge: ('gpt-4', 'single-math-v1')\n",
            " 55% 88/160 [06:21<06:34,  5.48s/it]question: 127, turn: 1, model: vicuna-7b-v1.5, score: 2, judge: ('gpt-4', 'single-math-v1')\n",
            " 56% 89/160 [06:23<05:52,  4.97s/it]question: 84, turn: 1, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1')\n",
            " 56% 90/160 [06:28<05:46,  4.94s/it]<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-m8ABNiJGMemgtsShkmm6M2va on tokens per min (TPM): Limit 10000, Used 8944, Requested 1110. Please try again in 324ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 130, turn: 2, model: vicuna-7b-v1.5, score: 2, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            " 57% 91/160 [06:37<06:31,  5.68s/it]question: 87, turn: 1, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1')\n",
            "question: 94, turn: 2, model: vicuna-7b-v1.5, score: 8, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            " 57% 92/160 [06:42<06:21,  5.61s/it]<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-m8ABNiJGMemgtsShkmm6M2va on tokens per min (TPM): Limit 10000, Used 9295, Requested 936. Please try again in 1.386s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 119, turn: 1, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-math-v1')\n",
            " 59% 94/160 [06:51<05:43,  5.21s/it]question: 85, turn: 2, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "question: 160, turn: 2, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "question: 93, turn: 1, model: vicuna-7b-v1.5, score: 7, judge: ('gpt-4', 'single-v1')\n",
            "<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-m8ABNiJGMemgtsShkmm6M2va on tokens per min (TPM): Limit 10000, Used 8875, Requested 1623. Please try again in 2.988s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 126, turn: 1, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-math-v1')\n",
            " 59% 95/160 [07:11<09:02,  8.34s/it]question: 95, turn: 1, model: vicuna-7b-v1.5, score: 6, judge: ('gpt-4', 'single-v1')\n",
            "<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-m8ABNiJGMemgtsShkmm6M2va on tokens per min (TPM): Limit 10000, Used 9390, Requested 1341. Please try again in 4.386s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 154, turn: 2, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            " 62% 99/160 [07:20<05:05,  5.01s/it]question: 92, turn: 2, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-m8ABNiJGMemgtsShkmm6M2va on tokens per min (TPM): Limit 10000, Used 8896, Requested 1341. Please try again in 1.422s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 108, turn: 2, model: vicuna-7b-v1.5, score: 2, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            "question: 157, turn: 1, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1')\n",
            "question: 92, turn: 1, model: vicuna-7b-v1.5, score: 8, judge: ('gpt-4', 'single-v1')\n",
            "question: 156, turn: 2, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            " 63% 101/160 [07:40<06:17,  6.39s/it]<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-m8ABNiJGMemgtsShkmm6M2va on tokens per min (TPM): Limit 10000, Used 9851, Requested 761. Please try again in 3.672s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 133, turn: 2, model: vicuna-7b-v1.5, score: 5, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            " 66% 106/160 [07:44<03:14,  3.60s/it]question: 108, turn: 1, model: vicuna-7b-v1.5, score: 2, judge: ('gpt-4', 'single-math-v1')\n",
            "question: 152, turn: 1, model: vicuna-7b-v1.5, score: 8, judge: ('gpt-4', 'single-v1')\n",
            "<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-m8ABNiJGMemgtsShkmm6M2va on tokens per min (TPM): Limit 10000, Used 9299, Requested 1787. Please try again in 6.516s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 146, turn: 1, model: vicuna-7b-v1.5, score: 8, judge: ('gpt-4', 'single-v1')\n",
            " 67% 107/160 [07:55<04:01,  4.56s/it]question: 116, turn: 1, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-math-v1')\n",
            "question: 82, turn: 1, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1')\n",
            "question: 101, turn: 2, model: vicuna-7b-v1.5, score: 2, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            "question: 127, turn: 2, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            " 69% 110/160 [08:27<05:34,  6.69s/it]<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-m8ABNiJGMemgtsShkmm6M2va on tokens per min (TPM): Limit 10000, Used 9334, Requested 809. Please try again in 858ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 119, turn: 2, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            " 71% 114/160 [08:34<03:36,  4.70s/it]question: 136, turn: 2, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "question: 85, turn: 1, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1')\n",
            "<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-m8ABNiJGMemgtsShkmm6M2va on tokens per min (TPM): Limit 10000, Used 9477, Requested 1092. Please try again in 3.414s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 153, turn: 1, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-v1')\n",
            " 72% 115/160 [08:42<03:47,  5.05s/it]question: 98, turn: 1, model: vicuna-7b-v1.5, score: 8, judge: ('gpt-4', 'single-v1')\n",
            "question: 147, turn: 2, model: vicuna-7b-v1.5, score: 6, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-m8ABNiJGMemgtsShkmm6M2va on tokens per min (TPM): Limit 10000, Used 9337, Requested 1092. Please try again in 2.574s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 135, turn: 2, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "question: 149, turn: 1, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1')\n",
            "question: 86, turn: 1, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-v1')\n",
            "<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-m8ABNiJGMemgtsShkmm6M2va on tokens per min (TPM): Limit 10000, Used 9129, Requested 1092. Please try again in 1.326s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 104, turn: 1, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-math-v1')\n",
            "<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-m8ABNiJGMemgtsShkmm6M2va on tokens per min (TPM): Limit 10000, Used 8799, Requested 1931. Please try again in 4.38s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-m8ABNiJGMemgtsShkmm6M2va on tokens per min (TPM): Limit 10000, Used 8202, Requested 1931. Please try again in 798ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 109, turn: 2, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            " 74% 118/160 [09:18<05:14,  7.50s/it]question: 157, turn: 2, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-m8ABNiJGMemgtsShkmm6M2va on tokens per min (TPM): Limit 10000, Used 8881, Requested 1931. Please try again in 4.872s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 81, turn: 1, model: vicuna-7b-v1.5, score: 8.5, judge: ('gpt-4', 'single-v1')\n",
            "question: 145, turn: 1, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1')\n",
            "<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-m8ABNiJGMemgtsShkmm6M2va on tokens per min (TPM): Limit 10000, Used 8185, Requested 1931. Please try again in 696ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 139, turn: 1, model: vicuna-7b-v1.5, score: 2, judge: ('gpt-4', 'single-v1')\n",
            "question: 145, turn: 2, model: vicuna-7b-v1.5, score: 2, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-m8ABNiJGMemgtsShkmm6M2va on tokens per min (TPM): Limit 10000, Used 8119, Requested 1931. Please try again in 300ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 142, turn: 1, model: vicuna-7b-v1.5, score: 4, judge: ('gpt-4', 'single-v1')\n",
            "question: 105, turn: 2, model: vicuna-7b-v1.5, score: 3, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            "question: 158, turn: 2, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-m8ABNiJGMemgtsShkmm6M2va on tokens per min (TPM): Limit 10000, Used 9455, Requested 1931. Please try again in 8.316s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 120, turn: 2, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            "<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-m8ABNiJGMemgtsShkmm6M2va on tokens per min (TPM): Limit 10000, Used 9624, Requested 701. Please try again in 1.95s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 124, turn: 2, model: vicuna-7b-v1.5, score: 2, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            " 78% 125/160 [10:15<04:34,  7.83s/it]question: 140, turn: 1, model: vicuna-7b-v1.5, score: 2, judge: ('gpt-4', 'single-v1')\n",
            "<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-m8ABNiJGMemgtsShkmm6M2va on tokens per min (TPM): Limit 10000, Used 9774, Requested 776. Please try again in 3.3s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 143, turn: 1, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1')\n",
            " 84% 135/160 [10:22<01:42,  4.11s/it]<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-m8ABNiJGMemgtsShkmm6M2va on tokens per min (TPM): Limit 10000, Used 9218, Requested 872. Please try again in 540ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 100, turn: 2, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            " 86% 137/160 [10:31<01:35,  4.17s/it]question: 83, turn: 2, model: vicuna-7b-v1.5, score: 4, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            " 86% 138/160 [10:37<01:35,  4.32s/it]<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-m8ABNiJGMemgtsShkmm6M2va on tokens per min (TPM): Limit 10000, Used 9012, Requested 1084. Please try again in 576ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 130, turn: 1, model: vicuna-7b-v1.5, score: 2, judge: ('gpt-4', 'single-math-v1')\n",
            " 87% 139/160 [10:38<01:23,  4.00s/it]<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-m8ABNiJGMemgtsShkmm6M2va on tokens per min (TPM): Limit 10000, Used 9095, Requested 1615. Please try again in 4.26s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-m8ABNiJGMemgtsShkmm6M2va on tokens per min (TPM): Limit 10000, Used 8495, Requested 1615. Please try again in 660ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 146, turn: 2, model: vicuna-7b-v1.5, score: 4, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            " 88% 140/160 [10:52<01:46,  5.33s/it]question: 123, turn: 1, model: vicuna-7b-v1.5, score: 8, judge: ('gpt-4', 'single-math-v1')\n",
            "question: 126, turn: 2, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            " 88% 141/160 [11:10<02:20,  7.38s/it]question: 136, turn: 1, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1')\n",
            "question: 82, turn: 2, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-m8ABNiJGMemgtsShkmm6M2va on tokens per min (TPM): Limit 10000, Used 8601, Requested 1733. Please try again in 2.004s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 128, turn: 1, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-math-v1')\n",
            " 89% 143/160 [11:19<01:49,  6.45s/it]<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-m8ABNiJGMemgtsShkmm6M2va on tokens per min (TPM): Limit 10000, Used 8804, Requested 1582. Please try again in 2.316s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-m8ABNiJGMemgtsShkmm6M2va on tokens per min (TPM): Limit 10000, Used 8857, Requested 1582. Please try again in 2.634s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 121, turn: 2, model: vicuna-7b-v1.5, score: 2, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            " 91% 146/160 [11:42<01:35,  6.86s/it]question: 150, turn: 1, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1')\n",
            "question: 123, turn: 2, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            " 92% 147/160 [11:51<01:34,  7.31s/it]question: 109, turn: 1, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-math-v1')\n",
            " 93% 149/160 [11:51<00:55,  5.04s/it]question: 89, turn: 2, model: vicuna-7b-v1.5, score: 4, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "question: 111, turn: 2, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-math-v1-multi-turn')\n",
            " 94% 150/160 [12:03<01:02,  6.26s/it]question: 121, turn: 1, model: vicuna-7b-v1.5, score: 8, judge: ('gpt-4', 'single-math-v1')\n",
            " 95% 152/160 [12:06<00:36,  4.58s/it]question: 88, turn: 2, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            " 96% 153/160 [12:07<00:26,  3.84s/it]question: 132, turn: 1, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1')\n",
            "question: 147, turn: 1, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-v1')\n",
            " 96% 154/160 [12:10<00:22,  3.69s/it]<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-m8ABNiJGMemgtsShkmm6M2va on tokens per min (TPM): Limit 10000, Used 9823, Requested 504. Please try again in 1.962s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 90, turn: 1, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1')\n",
            " 98% 156/160 [12:12<00:10,  2.72s/it]question: 118, turn: 1, model: vicuna-7b-v1.5, score: 1, judge: ('gpt-4', 'single-math-v1')\n",
            "<class 'openai.error.RateLimitError'> Rate limit reached for gpt-4 in organization org-m8ABNiJGMemgtsShkmm6M2va on tokens per min (TPM): Limit 10000, Used 9198, Requested 1436. Please try again in 3.804s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "question: 134, turn: 2, model: vicuna-7b-v1.5, score: 2, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            " 98% 157/160 [12:23<00:13,  4.37s/it]question: 158, turn: 1, model: vicuna-7b-v1.5, score: 10, judge: ('gpt-4', 'single-v1')\n",
            "question: 148, turn: 2, model: vicuna-7b-v1.5, score: 9, judge: ('gpt-4', 'single-v1-multi-turn')\n",
            "100% 160/160 [12:36<00:00,  4.73s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /src/FastChat/fastchat/llm_judge/show_result.py --model-list vicuna-7b-v1.5 vicuna-13b-v1.3 alpaca-13b llama-13b claude-v1 gpt-3.5-turbo gpt-4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrG5DYl2Cwau",
        "outputId": "0ab86b8c-1139-4845-b09c-69a69e1a1148"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mode: single\n",
            "Input file: data/mt_bench/model_judgment/gpt-4_single.jsonl\n",
            "\n",
            "########## First turn ##########\n",
            "                        score\n",
            "model           turn         \n",
            "gpt-4           1     8.95625\n",
            "claude-v1       1     8.15000\n",
            "gpt-3.5-turbo   1     8.07500\n",
            "vicuna-13b-v1.3 1     6.81250\n",
            "vicuna-7b-v1.5  1     6.71875\n",
            "alpaca-13b      1     4.97500\n",
            "llama-13b       1     3.26250\n",
            "\n",
            "########## Second turn ##########\n",
            "                       score\n",
            "model           turn        \n",
            "gpt-4           2     9.0250\n",
            "gpt-3.5-turbo   2     7.8125\n",
            "claude-v1       2     7.6500\n",
            "vicuna-13b-v1.3 2     5.9625\n",
            "vicuna-7b-v1.5  2     5.5125\n",
            "alpaca-13b      2     4.0875\n",
            "llama-13b       2     1.9500\n",
            "\n",
            "########## Average ##########\n",
            "                    score\n",
            "model                    \n",
            "gpt-4            8.990625\n",
            "gpt-3.5-turbo    7.943750\n",
            "claude-v1        7.900000\n",
            "vicuna-13b-v1.3  6.387500\n",
            "vicuna-7b-v1.5   6.115625\n",
            "alpaca-13b       4.531250\n",
            "llama-13b        2.606250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /src/FastChat/fastchat/llm_judge/show_result.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4GS9VrSCwX5",
        "outputId": "49cc42db-a6e1-4ad6-baf4-a8709423d4cd"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mode: single\n",
            "Input file: data/mt_bench/model_judgment/gpt-4_single.jsonl\n",
            "\n",
            "########## First turn ##########\n",
            "                                    score\n",
            "model                       turn         \n",
            "gpt-4                       1     8.95625\n",
            "claude-v1                   1     8.15000\n",
            "gpt-3.5-turbo               1     8.07500\n",
            "claude-instant-v1           1     7.80000\n",
            "vicuna-33b-v1.3             1     7.45625\n",
            "wizardlm-30b                1     7.13125\n",
            "wizardlm-13b                1     7.11875\n",
            "oasst-sft-7-llama-30b       1     7.10625\n",
            "Llama-2-13b-chat            1     7.06250\n",
            "tulu-30b                    1     7.01875\n",
            "Llama-2-70b-chat            1     6.98750\n",
            "guanaco-33b                 1     6.88125\n",
            "vicuna-13b-v1.3             1     6.81250\n",
            "guanaco-65b                 1     6.78125\n",
            "vicuna-7b-v1.5              1     6.71875\n",
            "palm-2-chat-bison-001       1     6.71250\n",
            "vicuna-7b-v1.3              1     6.69375\n",
            "mpt-30b-chat                1     6.67500\n",
            "nous-hermes-13b             1     6.43125\n",
            "Llama-2-7b-chat             1     6.41250\n",
            "baize-v2-13b                1     6.31875\n",
            "gpt4all-13b-snoozy          1     6.07500\n",
            "koala-13b                   1     6.07500\n",
            "mpt-7b-chat                 1     5.85000\n",
            "falcon-40b-instruct         1     5.81250\n",
            "mpt-30b-instruct            1     5.67500\n",
            "h2ogpt-oasst-open-llama-13b 1     5.51250\n",
            "chatglm-6b                  1     5.00000\n",
            "oasst-sft-4-pythia-12b      1     4.97500\n",
            "alpaca-13b                  1     4.97500\n",
            "rwkv-4-raven-14b            1     4.74375\n",
            "dolly-v2-12b                1     3.80000\n",
            "fastchat-t5-3b              1     3.39375\n",
            "llama-13b                   1     3.26250\n",
            "stablelm-tuned-alpha-7b     1     2.96875\n",
            "\n",
            "########## Second turn ##########\n",
            "                                     score\n",
            "model                       turn          \n",
            "gpt-4                       2     9.025000\n",
            "claude-instant-v1           2     8.012658\n",
            "gpt-3.5-turbo               2     7.812500\n",
            "claude-v1                   2     7.650000\n",
            "wizardlm-30b                2     6.887500\n",
            "vicuna-33b-v1.3             2     6.787500\n",
            "Llama-2-70b-chat            2     6.725000\n",
            "Llama-2-13b-chat            2     6.237500\n",
            "guanaco-33b                 2     6.175000\n",
            "Llama-2-7b-chat             2     6.125000\n",
            "mpt-30b-chat                2     6.112500\n",
            "palm-2-chat-bison-001       2     6.087500\n",
            "guanaco-65b                 2     6.037500\n",
            "vicuna-13b-v1.3             2     5.962500\n",
            "tulu-30b                    2     5.850000\n",
            "oasst-sft-7-llama-30b       2     5.712500\n",
            "wizardlm-13b                2     5.587500\n",
            "vicuna-7b-v1.5              2     5.512500\n",
            "vicuna-7b-v1.3              2     5.300000\n",
            "baize-v2-13b                2     5.181250\n",
            "mpt-7b-chat                 2     5.063291\n",
            "gpt4all-13b-snoozy          2     4.822785\n",
            "mpt-30b-instruct            2     4.762500\n",
            "nous-hermes-13b             2     4.664557\n",
            "koala-13b                   2     4.625000\n",
            "falcon-40b-instruct         2     4.525000\n",
            "alpaca-13b                  2     4.087500\n",
            "chatglm-6b                  2     4.000000\n",
            "h2ogpt-oasst-open-llama-13b 2     3.737500\n",
            "oasst-sft-4-pythia-12b      2     3.662500\n",
            "rwkv-4-raven-14b            2     3.225000\n",
            "dolly-v2-12b                2     2.750000\n",
            "fastchat-t5-3b              2     2.687500\n",
            "stablelm-tuned-alpha-7b     2     2.537500\n",
            "llama-13b                   2     1.950000\n",
            "\n",
            "########## Average ##########\n",
            "                                score\n",
            "model                                \n",
            "gpt-4                        8.990625\n",
            "gpt-3.5-turbo                7.943750\n",
            "claude-instant-v1            7.905660\n",
            "claude-v1                    7.900000\n",
            "vicuna-33b-v1.3              7.121875\n",
            "wizardlm-30b                 7.009375\n",
            "Llama-2-70b-chat             6.856250\n",
            "Llama-2-13b-chat             6.650000\n",
            "guanaco-33b                  6.528125\n",
            "tulu-30b                     6.434375\n",
            "oasst-sft-7-llama-30b        6.409375\n",
            "guanaco-65b                  6.409375\n",
            "palm-2-chat-bison-001        6.400000\n",
            "mpt-30b-chat                 6.393750\n",
            "vicuna-13b-v1.3              6.387500\n",
            "wizardlm-13b                 6.353125\n",
            "Llama-2-7b-chat              6.268750\n",
            "vicuna-7b-v1.5               6.115625\n",
            "vicuna-7b-v1.3               5.996875\n",
            "baize-v2-13b                 5.750000\n",
            "nous-hermes-13b              5.553459\n",
            "mpt-7b-chat                  5.459119\n",
            "gpt4all-13b-snoozy           5.452830\n",
            "koala-13b                    5.350000\n",
            "mpt-30b-instruct             5.218750\n",
            "falcon-40b-instruct          5.168750\n",
            "h2ogpt-oasst-open-llama-13b  4.625000\n",
            "alpaca-13b                   4.531250\n",
            "chatglm-6b                   4.500000\n",
            "oasst-sft-4-pythia-12b       4.318750\n",
            "rwkv-4-raven-14b             3.984375\n",
            "dolly-v2-12b                 3.275000\n",
            "fastchat-t5-3b               3.040625\n",
            "stablelm-tuned-alpha-7b      2.753125\n",
            "llama-13b                    2.606250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /src/FastChat/fastchat/llm_judge/show_result.py --mode pairwise-all"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nANjQxMhkZXK",
        "outputId": "746aeab9-391f-4583-fa22-f4748f26bd1d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mode: pairwise-all\n",
            "Input file: data/mt_bench/model_judgment/gpt-4_pair.jsonl\n",
            "                              win  loss  ...  loss_rate  win_rate_adjusted\n",
            "model                                    ...                              \n",
            "gpt-4                         111     7  ...   0.043750           0.825000\n",
            "gpt-3.5-turbo                2854   771  ...   0.160759           0.717160\n",
            "claude-v1                      75    27  ...   0.168750           0.650000\n",
            "vicuna-33b-v1.3                70    42  ...   0.262500           0.587500\n",
            "claude-instant-v1              64    40  ...   0.250000           0.575000\n",
            "wizardlm-30b                   37    63  ...   0.393750           0.418750\n",
            "guanaco-65b                    38    68  ...   0.425000           0.406250\n",
            "guanaco-33b                    42    72  ...   0.450000           0.406250\n",
            "vicuna-13b-v1.3                33    73  ...   0.456250           0.375000\n",
            "mpt-30b-chat                   29    78  ...   0.487500           0.346875\n",
            "vicuna-7b-v1.3                 30    84  ...   0.525000           0.331250\n",
            "wizardlm-13b                   27    81  ...   0.506250           0.331250\n",
            "tulu-30b                       29    92  ...   0.575000           0.303125\n",
            "oasst-sft-7-llama-30b          23    88  ...   0.550000           0.296875\n",
            "baize-v2-13b                   21   101  ...   0.631250           0.250000\n",
            "palm-2-chat-bison-001          18   102  ...   0.637500           0.237500\n",
            "mpt-7b-chat                    10   102  ...   0.637500           0.212500\n",
            "nous-hermes-13b                12   104  ...   0.650000           0.212500\n",
            "gpt4all-13b-snoozy             14   108  ...   0.675000           0.206250\n",
            "h2ogpt-oasst-open-llama-13b    19   118  ...   0.742138           0.188679\n",
            "koala-13b                      10   110  ...   0.687500           0.187500\n",
            "falcon-40b-instruct            10   116  ...   0.725000           0.168750\n",
            "mpt-30b-instruct                7   120  ...   0.750000           0.146875\n",
            "chatglm-6b                      6   124  ...   0.775000           0.131250\n",
            "oasst-sft-4-pythia-12b          8   128  ...   0.800000           0.125000\n",
            "alpaca-13b                      8   130  ...   0.812500           0.118750\n",
            "rwkv-4-raven-14b                6   128  ...   0.800000           0.118750\n",
            "fastchat-t5-3b                  5   132  ...   0.835443           0.098101\n",
            "dolly-v2-12b                    5   138  ...   0.862500           0.084375\n",
            "stablelm-tuned-alpha-7b         1   137  ...   0.861635           0.072327\n",
            "llama-13b                       3   141  ...   0.881250           0.068750\n",
            "\n",
            "[31 rows x 6 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget /src/FastChat/data/mt_bench/model_judgment/gpt-4_single.jsonl\n",
        "!wget https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_judgment/gpt-4_pair.jsonl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "er1Jwk0emuvR",
        "outputId": "51c2d7f5-86c6-49e9-e045-e44b932aa9e4"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/src/FastChat/data/mt_bench/model_judgment/gpt-4_single.jsonl: Scheme missing.\n",
            "--2025-04-16 03:19:41--  https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_judgment/gpt-4_pair.jsonl\n",
            "Resolving huggingface.co (huggingface.co)... 18.164.174.118, 18.164.174.17, 18.164.174.55, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.164.174.118|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.hf.co/repos/12/2b/122bd8e9eccbb3acc98acf73e0ecef3c96f24dcdb5f6639074ed304eb19f9cd4/d662c0b7d1d297f0494fcb4cc09fe8f054fa22d75deb4754a483a921984bc585?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27gpt-4_pair.jsonl%3B+filename%3D%22gpt-4_pair.jsonl%22%3B&Expires=1744777181&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NDc3NzE4MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy8xMi8yYi8xMjJiZDhlOWVjY2JiM2FjYzk4YWNmNzNlMGVjZWYzYzk2ZjI0ZGNkYjVmNjYzOTA3NGVkMzA0ZWIxOWY5Y2Q0L2Q2NjJjMGI3ZDFkMjk3ZjA0OTRmY2I0Y2MwOWZlOGYwNTRmYTIyZDc1ZGViNDc1NGE0ODNhOTIxOTg0YmM1ODU%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=XaaksgS%7ELGnqSNY19%7EOeoEuMjsS8Dt3vX5rbYYFkThmuubnZdbT9b%7Emd8QnF4bcSrEixRZwM36Y%7E4fsz3oZlq5gAbpo8lPhojy7WaVityzyIniZCSTvJtXsxTZJ0%7Ee6JoYCpRADIssTQHVASPQwskIAluGrk98EbmXngsUvMA0atC0GbytUbh%7EmsOR7LHqVhURGouUVz-VPv8xrKU8w71TNN-sSMlLbVZreu72k0dOpDv26h060v%7EFFOW3%7ELu8MK2oN%7ERcyl-feGA85mtRmhT71kEuZW4pZTJARmyXqjC15jsufMbnpgmgKwt7br8nOuHfJ8HokLKRP3zWGxi-Bc2g__&Key-Pair-Id=K3RPWS32NSSJCE [following]\n",
            "--2025-04-16 03:19:41--  https://cdn-lfs.hf.co/repos/12/2b/122bd8e9eccbb3acc98acf73e0ecef3c96f24dcdb5f6639074ed304eb19f9cd4/d662c0b7d1d297f0494fcb4cc09fe8f054fa22d75deb4754a483a921984bc585?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27gpt-4_pair.jsonl%3B+filename%3D%22gpt-4_pair.jsonl%22%3B&Expires=1744777181&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NDc3NzE4MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy8xMi8yYi8xMjJiZDhlOWVjY2JiM2FjYzk4YWNmNzNlMGVjZWYzYzk2ZjI0ZGNkYjVmNjYzOTA3NGVkMzA0ZWIxOWY5Y2Q0L2Q2NjJjMGI3ZDFkMjk3ZjA0OTRmY2I0Y2MwOWZlOGYwNTRmYTIyZDc1ZGViNDc1NGE0ODNhOTIxOTg0YmM1ODU%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=XaaksgS%7ELGnqSNY19%7EOeoEuMjsS8Dt3vX5rbYYFkThmuubnZdbT9b%7Emd8QnF4bcSrEixRZwM36Y%7E4fsz3oZlq5gAbpo8lPhojy7WaVityzyIniZCSTvJtXsxTZJ0%7Ee6JoYCpRADIssTQHVASPQwskIAluGrk98EbmXngsUvMA0atC0GbytUbh%7EmsOR7LHqVhURGouUVz-VPv8xrKU8w71TNN-sSMlLbVZreu72k0dOpDv26h060v%7EFFOW3%7ELu8MK2oN%7ERcyl-feGA85mtRmhT71kEuZW4pZTJARmyXqjC15jsufMbnpgmgKwt7br8nOuHfJ8HokLKRP3zWGxi-Bc2g__&Key-Pair-Id=K3RPWS32NSSJCE\n",
            "Resolving cdn-lfs.hf.co (cdn-lfs.hf.co)... 3.169.231.4, 3.169.231.87, 3.169.231.115, ...\n",
            "Connecting to cdn-lfs.hf.co (cdn-lfs.hf.co)|3.169.231.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 48043462 (46M) [binary/octet-stream]\n",
            "Saving to: ‘gpt-4_pair.jsonl.1’\n",
            "\n",
            "gpt-4_pair.jsonl.1  100%[===================>]  45.82M   179MB/s    in 0.3s    \n",
            "\n",
            "2025-04-16 03:19:42 (179 MB/s) - ‘gpt-4_pair.jsonl.1’ saved [48043462/48043462]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U plotly kaleido"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_470j7Xbmur0",
        "outputId": "6c7739df-cdff-482f-8619-2ec2fc291679"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Collecting plotly\n",
            "  Downloading plotly-6.0.1-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting kaleido\n",
            "  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl.metadata (15 kB)\n",
            "Requirement already satisfied: narwhals>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from plotly) (1.34.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly) (24.2)\n",
            "Downloading plotly-6.0.1-py3-none-any.whl (14.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m102.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: kaleido, plotly\n",
            "  Attempting uninstall: plotly\n",
            "    Found existing installation: plotly 5.24.1\n",
            "    Uninstalling plotly-5.24.1:\n",
            "      Successfully uninstalled plotly-5.24.1\n",
            "Successfully installed kaleido-0.2.1 plotly-6.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "\n",
        "CATEGORIES = [\"Writing\", \"Roleplay\", \"Reasoning\", \"Math\", \"Coding\", \"Extraction\", \"STEM\", \"Humanities\"]\n",
        "\n",
        "\n",
        "def get_model_df():\n",
        "    cnt = 0\n",
        "    q2result = []\n",
        "    fin = open(\"/src/FastChat/data/mt_bench/model_judgment/gpt-4_single.jsonl\", \"r\")\n",
        "    for line in fin:\n",
        "        obj = json.loads(line)\n",
        "        obj[\"category\"] = CATEGORIES[(obj[\"question_id\"]-81)//10]\n",
        "        q2result.append(obj)\n",
        "    df = pd.DataFrame(q2result)\n",
        "    return df\n",
        "\n",
        "def toggle(res_str):\n",
        "    if res_str == \"win\":\n",
        "        return \"loss\"\n",
        "    elif res_str == \"loss\":\n",
        "        return \"win\"\n",
        "    return \"tie\"\n",
        "\n",
        "def get_model_df_pair():\n",
        "    fin = open(\"gpt-4_pair.jsonl\", \"r\")\n",
        "    cnt = 0\n",
        "    q2result = []\n",
        "    for line in fin:\n",
        "        obj = json.loads(line)\n",
        "\n",
        "        result = {}\n",
        "        result[\"qid\"] = str(obj[\"question_id\"])\n",
        "        result[\"turn\"] = str(obj[\"turn\"])\n",
        "        if obj[\"g1_winner\"] == \"model_1\" and obj[\"g2_winner\"] == \"model_1\":\n",
        "            result[\"result\"] = \"win\"\n",
        "        elif obj[\"g1_winner\"] == \"model_2\" and obj[\"g2_winner\"] == \"model_2\":\n",
        "            result[\"result\"] = \"loss\"\n",
        "        else:\n",
        "            result[\"result\"] = \"tie\"\n",
        "        result[\"category\"] = CATEGORIES[(obj[\"question_id\"]-81)//10]\n",
        "        result[\"model\"] = obj[\"model_1\"]\n",
        "        q2result.append(result)\n",
        "\n",
        "    df = pd.DataFrame(q2result)\n",
        "\n",
        "    return df\n",
        "\n",
        "df = get_model_df()\n",
        "df_pair = get_model_df_pair()"
      ],
      "metadata": {
        "id": "EhG2-42tmuor"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "SyBVbCN8muk1",
        "outputId": "1aa57be4-5464-4a53-c7e5-912660cd0252"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      question_id           model                          judge  \\\n",
              "0              81      alpaca-13b             [gpt-4, single-v1]   \n",
              "1              81      alpaca-13b  [gpt-4, single-v1-multi-turn]   \n",
              "2              82      alpaca-13b             [gpt-4, single-v1]   \n",
              "3              82      alpaca-13b  [gpt-4, single-v1-multi-turn]   \n",
              "4              83      alpaca-13b             [gpt-4, single-v1]   \n",
              "...           ...             ...                            ...   \n",
              "5595           90  vicuna-7b-v1.5             [gpt-4, single-v1]   \n",
              "5596          118  vicuna-7b-v1.5        [gpt-4, single-math-v1]   \n",
              "5597          134  vicuna-7b-v1.5  [gpt-4, single-v1-multi-turn]   \n",
              "5598          158  vicuna-7b-v1.5             [gpt-4, single-v1]   \n",
              "5599          148  vicuna-7b-v1.5  [gpt-4, single-v1-multi-turn]   \n",
              "\n",
              "                                            user_prompt  \\\n",
              "0     [Instruction]\\nPlease act as an impartial judg...   \n",
              "1     <|The Start of Assistant A's Conversation with...   \n",
              "2     [Instruction]\\nPlease act as an impartial judg...   \n",
              "3     <|The Start of Assistant A's Conversation with...   \n",
              "4     [Instruction]\\nPlease act as an impartial judg...   \n",
              "...                                                 ...   \n",
              "5595  [Instruction]\\nPlease act as an impartial judg...   \n",
              "5596  [Instruction]\\nPlease act as an impartial judg...   \n",
              "5597  <|The Start of Assistant A's Conversation with...   \n",
              "5598  [Instruction]\\nPlease act as an impartial judg...   \n",
              "5599  <|The Start of Assistant A's Conversation with...   \n",
              "\n",
              "                                               judgment  score  turn  \\\n",
              "0     The assistant's response is relevant and accur...    7.0     1   \n",
              "1     The assistant failed to follow the user's inst...    1.0     2   \n",
              "2     The assistant's response is concise, professio...    8.0     1   \n",
              "3     The AI assistant's self-evaluation is accurate...    7.0     2   \n",
              "4     The assistant's response is very relevant and ...   10.0     1   \n",
              "...                                                 ...    ...   ...   \n",
              "5595  The assistant's response is excellent. It has ...   10.0     1   \n",
              "5596  The assistant's answer is incorrect. The assis...    1.0     1   \n",
              "5597  The assistant's response is incorrect. The ass...    2.0     2   \n",
              "5598  The assistant's response is highly relevant, a...   10.0     1   \n",
              "5599  The assistant's response is relevant, accurate...    9.0     2   \n",
              "\n",
              "            tstamp    category  \n",
              "0     1.687222e+09     Writing  \n",
              "1     1.687222e+09     Writing  \n",
              "2     1.687224e+09     Writing  \n",
              "3     1.687223e+09     Writing  \n",
              "4     1.687222e+09     Writing  \n",
              "...            ...         ...  \n",
              "5595  1.744772e+09     Writing  \n",
              "5596  1.744772e+09        Math  \n",
              "5597  1.744772e+09  Extraction  \n",
              "5598  1.744772e+09  Humanities  \n",
              "5599  1.744772e+09        STEM  \n",
              "\n",
              "[5600 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bdd7bd7d-c5d1-42bc-8418-1abd87a42960\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question_id</th>\n",
              "      <th>model</th>\n",
              "      <th>judge</th>\n",
              "      <th>user_prompt</th>\n",
              "      <th>judgment</th>\n",
              "      <th>score</th>\n",
              "      <th>turn</th>\n",
              "      <th>tstamp</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>81</td>\n",
              "      <td>alpaca-13b</td>\n",
              "      <td>[gpt-4, single-v1]</td>\n",
              "      <td>[Instruction]\\nPlease act as an impartial judg...</td>\n",
              "      <td>The assistant's response is relevant and accur...</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.687222e+09</td>\n",
              "      <td>Writing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>81</td>\n",
              "      <td>alpaca-13b</td>\n",
              "      <td>[gpt-4, single-v1-multi-turn]</td>\n",
              "      <td>&lt;|The Start of Assistant A's Conversation with...</td>\n",
              "      <td>The assistant failed to follow the user's inst...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1.687222e+09</td>\n",
              "      <td>Writing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>82</td>\n",
              "      <td>alpaca-13b</td>\n",
              "      <td>[gpt-4, single-v1]</td>\n",
              "      <td>[Instruction]\\nPlease act as an impartial judg...</td>\n",
              "      <td>The assistant's response is concise, professio...</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.687224e+09</td>\n",
              "      <td>Writing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>82</td>\n",
              "      <td>alpaca-13b</td>\n",
              "      <td>[gpt-4, single-v1-multi-turn]</td>\n",
              "      <td>&lt;|The Start of Assistant A's Conversation with...</td>\n",
              "      <td>The AI assistant's self-evaluation is accurate...</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1.687223e+09</td>\n",
              "      <td>Writing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>83</td>\n",
              "      <td>alpaca-13b</td>\n",
              "      <td>[gpt-4, single-v1]</td>\n",
              "      <td>[Instruction]\\nPlease act as an impartial judg...</td>\n",
              "      <td>The assistant's response is very relevant and ...</td>\n",
              "      <td>10.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.687222e+09</td>\n",
              "      <td>Writing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5595</th>\n",
              "      <td>90</td>\n",
              "      <td>vicuna-7b-v1.5</td>\n",
              "      <td>[gpt-4, single-v1]</td>\n",
              "      <td>[Instruction]\\nPlease act as an impartial judg...</td>\n",
              "      <td>The assistant's response is excellent. It has ...</td>\n",
              "      <td>10.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.744772e+09</td>\n",
              "      <td>Writing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5596</th>\n",
              "      <td>118</td>\n",
              "      <td>vicuna-7b-v1.5</td>\n",
              "      <td>[gpt-4, single-math-v1]</td>\n",
              "      <td>[Instruction]\\nPlease act as an impartial judg...</td>\n",
              "      <td>The assistant's answer is incorrect. The assis...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.744772e+09</td>\n",
              "      <td>Math</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5597</th>\n",
              "      <td>134</td>\n",
              "      <td>vicuna-7b-v1.5</td>\n",
              "      <td>[gpt-4, single-v1-multi-turn]</td>\n",
              "      <td>&lt;|The Start of Assistant A's Conversation with...</td>\n",
              "      <td>The assistant's response is incorrect. The ass...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1.744772e+09</td>\n",
              "      <td>Extraction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5598</th>\n",
              "      <td>158</td>\n",
              "      <td>vicuna-7b-v1.5</td>\n",
              "      <td>[gpt-4, single-v1]</td>\n",
              "      <td>[Instruction]\\nPlease act as an impartial judg...</td>\n",
              "      <td>The assistant's response is highly relevant, a...</td>\n",
              "      <td>10.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.744772e+09</td>\n",
              "      <td>Humanities</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5599</th>\n",
              "      <td>148</td>\n",
              "      <td>vicuna-7b-v1.5</td>\n",
              "      <td>[gpt-4, single-v1-multi-turn]</td>\n",
              "      <td>&lt;|The Start of Assistant A's Conversation with...</td>\n",
              "      <td>The assistant's response is relevant, accurate...</td>\n",
              "      <td>9.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1.744772e+09</td>\n",
              "      <td>STEM</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5600 rows × 9 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bdd7bd7d-c5d1-42bc-8418-1abd87a42960')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bdd7bd7d-c5d1-42bc-8418-1abd87a42960 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bdd7bd7d-c5d1-42bc-8418-1abd87a42960');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9ab441fc-dff7-437d-909b-a08ed7bb0459\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9ab441fc-dff7-437d-909b-a08ed7bb0459')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9ab441fc-dff7-437d-909b-a08ed7bb0459 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_7b473c99-3c47-49f3-ac86-c83f34ae208e\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_7b473c99-3c47-49f3-ac86-c83f34ae208e button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5600,\n  \"fields\": [\n    {\n      \"column\": \"question_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 23,\n        \"min\": 81,\n        \"max\": 160,\n        \"num_unique_values\": 80,\n        \"samples\": [\n          111,\n          81,\n          103\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 35,\n        \"samples\": [\n          \"vicuna-13b-v1.3\",\n          \"h2ogpt-oasst-open-llama-13b\",\n          \"stablelm-tuned-alpha-7b\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"judge\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"user_prompt\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5506,\n        \"samples\": [\n          \"[Instruction]\\nPlease act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below. Your evaluation should consider correctness and helpfulness. You will be given a reference answer and the assistant's answer. Begin your evaluation by comparing the assistant's answer with the reference answer. Identify and correct any mistakes. Be as objective as possible. After providing your explanation, you must rate the response on a scale of 1 to 10 by strictly following this format: \\\"[[rating]]\\\", for example: \\\"Rating: [[5]]\\\".\\n\\n[Question]\\nHow many integers are in the solution of the inequality |x + 5| < 10\\n\\n[The Start of Reference Answer]\\nTo solve the inequality |x + 5| < 10, we can break it down into two separate inequalities:\\n\\n1) x + 5 < 10\\n2) -(x + 5) < 10\\n\\nNow, let's solve each inequality:\\n\\n1) x + 5 < 10\\nSubtract 5 from both sides:\\nx < 5\\n\\n2) -(x + 5) < 10\\nDistribute the negative sign:\\n-x - 5 < 10\\nAdd 5 to both sides:\\n-x < 15\\nMultiply both sides by -1 (and remember to flip the inequality sign):\\nx > -15\\n\\nNow, we can combine the two inequalities:\\n-15 < x < 5\\n\\nTo find the number of integers in this range, we can count them:\\n-14, -13, -12, -11, -10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4\\n\\nThere are 19 integers in the solution of the inequality |x + 5| < 10.\\n[The End of Reference Answer]\\n\\n[The Start of Assistant's Answer]\\nTo solve the inequality |x + 5| < 10, we need to find all integers x that satisfy the condition. The absolute value function gives us the distance of a number from 0. So, we need to find all integers x for which the distance between x + 5 and 0 is less than 10.\\n\\nThere are two cases to consider:\\n\\n1. x + 5 > 0\\nIn this case, we need to find all integers x such that x > -5. The integer numbers greater than -5 are -4, -3, -2, -1, 0, 1, 2, 3, and 4. So, there are 8 integers in this case.\\n2. x + 5 < 0\\nIn this case, we need to find all integers x such that x < -5. The integer numbers less than -5 are -6, -7, -8, -9, and -10. So, there are 5 integers in this case.\\n\\nNow, we combine the results for both cases:\\n\\nNumber of integers in the solution of |x + 5| < 10 = 8 + 5 = 13\\n\\nSo, there are 13 integers in the solution of the inequality |x + 5| < 10.\\n[The End of Assistant's Answer]\",\n          \"<|The Start of Reference Answer|>\\n\\n### User:\\nWrite a simple website in HTML. When a user clicks the button, it shows a random joke from a list of 4 jokes.\\n\\n### Reference answer:\\n<!DOCTYPE html>\\n<html lang=\\\"en\\\">\\n<head>\\n    <meta charset=\\\"UTF-8\\\">\\n    <meta name=\\\"viewport\\\" content=\\\"width=device-width, initial-scale=1.0\\\">\\n    <title>Random Joke Generator</title>\\n    <style>\\n        body {\\n            font-family: Arial, sans-serif;\\n            text-align: center;\\n            background-color: #f0f0f0;\\n        }\\n        button {\\n            font-size: 20px;\\n            padding: 10px 20px;\\n            cursor: pointer;\\n        }\\n    </style>\\n    <script>\\n        function showRandomJoke() {\\n            const jokes = [\\n                \\\"Why don't scientists trust atoms? Because they make up everything!\\\",\\n                \\\"Why did the chicken go to the seance? To get to the other side.\\\",\\n                \\\"Why don't some couples go to the gym? Because some relationships don't work out.\\\",\\n                \\\"Did you hear about the mathematician who's afraid of negative numbers? He'll stop at nothing to avoid them!\\\"\\n            ];\\n\\n            const randomIndex = Math.floor(Math.random() * jokes.length);\\n            const randomJoke = jokes[randomIndex];\\n            document.getElementById(\\\"jokeDisplay\\\").innerHTML = randomJoke;\\n        }\\n    </script>\\n</head>\\n<body>\\n    <h1>Random Joke Generator</h1>\\n    <button onclick=\\\"showRandomJoke()\\\">Show me a joke!</button>\\n    <p id=\\\"jokeDisplay\\\"></p>\\n</body>\\n</html>\\n\\n### User:\\nHow to use CSS to change the color of jokes to red?\\n\\n### Reference answer:\\nYou can use CSS to change the color of the jokes to red by adding a style rule for the paragraph element where the joke is displayed. In the `<style>` section of the HTML code, add a rule for the `#jokeDisplay` element to set the `color` property to red:\\n\\n```html\\n<!DOCTYPE html>\\n<html lang=\\\"en\\\">\\n<head>\\n    <meta charset=\\\"UTF-8\\\">\\n    <meta name=\\\"viewport\\\" content=\\\"width=device-width, initial-scale=1.0\\\">\\n    <title>Random Joke Generator</title>\\n    <style>\\n        body {\\n            font-family: Arial, sans-serif;\\n            text-align: center;\\n            background-color: #f0f0f0;\\n        }\\n        button {\\n            font-size: 20px;\\n            padding: 10px 20px;\\n            cursor: pointer;\\n        }\\n        /* Add this rule to change the joke color to red */\\n        #jokeDisplay {\\n            color: red;\\n        }\\n    </style>\\n    <script>\\n        function showRandomJoke() {\\n            const jokes = [\\n                \\\"Why don't scientists trust atoms? Because they make up everything!\\\",\\n                \\\"Why did the chicken go to the seance? To get to the other side.\\\",\\n                \\\"Why don't some couples go to the gym? Because some relationships don't work out.\\\",\\n                \\\"Did you hear about the mathematician who's afraid of negative numbers? He'll stop at nothing to avoid them!\\\"\\n            ];\\n\\n            const randomIndex = Math.floor(Math.random() * jokes.length);\\n            const randomJoke = jokes[randomIndex];\\n            document.getElementById(\\\"jokeDisplay\\\").innerHTML = randomJoke;\\n        }\\n    </script>\\n</head>\\n<body>\\n    <h1>Random Joke Generator</h1>\\n    <button onclick=\\\"showRandomJoke()\\\">Show me a joke!</button>\\n    <p id=\\\"jokeDisplay\\\"></p>\\n</body>\\n</html>\\n```\\n\\nNow, when a joke is displayed, its text color will be red.\\n\\n<|The End of Reference Answer|>\\n\\n\\n<|The Start of Assistant A's Conversation with User|>\\n\\n### User:\\nWrite a simple website in HTML. When a user clicks the button, it shows a random joke from a list of 4 jokes.\\n\\n### Assistant A:\\nHere's a simple HTML website that displays a random joke when a user clicks a button:```html<!DOCTYPE html><html><head>\\n<title>Jokes and Fun</title>\\n<meta charset=\\\"UTF-8\\\">\\n<meta name=\\\"viewport\\\" content=\\\"width=device-width, initial-scale=1.0\\\">\\n<meta name=\\\"viewport\\\" content=\\\"height=device-height, initial-scale=1.0\\\">\\n<meta name=\\\"viewport\\\" content=\\\"width=device-width, initial-scale=1.0\\\">\\n<meta name=\\\"viewport\\\" content=\\\"height=device-height, initial-scale=1.0\\\">\\n<meta name=\\\"viewport\\\" content=\\\"height=device-height, initial-scale=1.0\\\">\\n<meta name=\\\"viewport\\\" content=\\\"height=device-height, initial-scale=1.0\\\">\\n<meta name=\\\"viewport\\\" content=\\\"height=device-height, initial-scale=1.0\\\">\\n<meta name=\\\"viewport\\\" content=\\\"height=device-height, initial-scale=1.0\\\">\\n<meta name=\\\"viewport\\\" content=\\\"height=device-height, initial-scale=1.0\\\">\\n<meta name=\\\"viewport\\\" content=\\\"height=device-height, initial-scale=1.0\\\">\\n<meta name=\\\"viewport\\\" content=\\\"height=device-height, initial-scale=1.0\\\">\\n<meta name=\\\"viewport\\\" content=\\\"height=device-height, initial-scale=1.0\\\">\\n<meta name=\\\"viewport\\\" content=\\\"height=device-height, initial-scale=1.0\\\">\\n<meta name=\\\"viewport\\\" content=\\\"height=device-height, initial-scale=1.0\\\">\\n<meta name=\\\"viewport\\\" content=\\\"height=device-height, initial-scale=1.0\\\">\\n<meta name=\\\"viewport\\\" content=\\\"height=device-height, initial-scale=1.0\\\">\\n<meta name=\\\"viewport\\\" content=\\\"height=device-height, initial-scale=1.0\\\">\\n<meta name=\\\"viewport\\\" content=\\\"height=device-height, initial-scale=1.0\\\">\\n<meta name=\\\"viewport\\\" content=\\\"height=device-height, initial-scale=1.0\\\">\\n<meta name=\\\"viewport\\\" content=\\\"height=device-height, initial-scale=1.0\\\">\\n<meta name=\\\"viewport\\\" content=\\\"height=device-height, initial-scale=1.0\\\">\\n<meta name=\\\"viewport\\\" content=\\\"height=device-height, initial-scale=1.0\\\">\\n<meta name=\\\"viewport\\\" content=\\\"height=device-height, initial-scale=1.0\\\">\\n<meta name=\\\"viewport\\\" content=\\\"height=device-height, initial-scale=1.0\\\">\\n<meta name=\\\"viewport\\\" content=\\\"height=device-height, initial-scale=1.0\\\">\\n<meta name=\\\"viewport\\\" content=\\\"height=device-height, initial-scale=1.0\\\">\\n<meta name=\\\"viewport\\\" content=\\\"height=device-height, initial-scale=1.0\\\">\\n<meta name=\\\"viewport\\\" content=\\\"height=device-height, initial-scale=1.0\\\">\\n<meta name=\\\"viewport\\\" content=\\\"height=device-height, initial-scale=1.0\\\">\\n<meta name=\\\"viewport\\\" content=\\\"height=device-height, initial-scale=1.0\\\">\\n<meta name=\\\"viewport\\\" content=\\\"height=device-height, initial-scale=1.0\\\">\\n<meta name=\\\"viewport\\\" content=\\\"height=device-height, initial-scale=1.0\\\">\\n<meta name=\\n\\n### User:\\nHow to use CSS to change the color of jokes to red?\\n\\n### Assistant A:\\nTo change the color of the jokes to red using CSS, you can add the following code to the HTML file:```html<!DOCTYPE html><html><head>\\n<title>Jokes and Fun</title>\\n<h1>Jokes and Fun</h1>\\n<p>A man named Jack lived in a cave for a year before he discovered a way to make fire with a stick.</p>\\n<p>A woman named Mary lived in a cave for a year before she discovered a way to make fire with a stick.</p>\\n<p>A man named Jack lived in a cave for a year before he discovered a way to make fire with a stick.</p>\\n<p>A woman named Mary lived in a cave for a year before she discovered a way to make fire with a stick.</p>\\n<p>A man named Jack lived in a cave for a year before he discovered a way to make fire with a stick.</p>\\n<p>A woman named Mary lived in a cave for a year before she discovered a way to make fire with a stick.</p>\\n<p>A man named Jack lived in a cave for a year before he discovered a way to make fire with a stick.</p>\\n<p>A woman named Mary lived in a cave for a year before she discovered a way to make fire with a stick.</p>\\n<p>A man named Jack lived in a cave for a year before he discovered a way to make fire with a stick.</p>\\n<p>A woman named Mary lived in a cave for a year before she discovered a way to make fire with a stick.</p>\\n<p>A man named Jack lived in a cave for a year before he discovered a way to make fire with a stick.</p>\\n<p>A woman named Mary lived in a cave for a year before she discovered a way to make fire with a stick.</p>\\n<p>A man named Jack lived in a cave for a year before he discovered a way to make fire with a stick.</p>\\n<p>A woman named Mary lived in a cave for a year before she discovered a way to make fire with a stick.</p>\\n<p>A man named Jack lived in a cave for a year before he discovered a way to make fire with a stick.</p>\\n<p>A woman named Mary lived in a cave for a year before she discovered a way to make fir\\n\\n<|The End of Assistant A's Conversation with User|>\",\n          \"<|The Start of Reference Answer|>\\n\\n### User:\\nYou are given two sorted lists of size m and n. Implement a function to find the kth smallest element in the union of the two lists with linear complexity.\\n\\n### Reference answer:\\nTo find the kth smallest element in the union of two sorted lists with linear complexity, you can use a two-pointer approach. Here's a Python function that implements this approach:\\n\\n```python\\ndef find_kth_smallest(arr1, arr2, k):\\n    m, n = len(arr1), len(arr2)\\n    i, j = 0, 0\\n\\n    while i < m and j < n:\\n        if arr1[i] < arr2[j]:\\n            k -= 1\\n            if k == 0:\\n                return arr1[i]\\n            i += 1\\n        else:\\n            k -= 1\\n            if k == 0:\\n                return arr2[j]\\n            j += 1\\n\\n    while i < m:\\n        k -= 1\\n        if k == 0:\\n            return arr1[i]\\n        i += 1\\n\\n    while j < n:\\n        k -= 1\\n        if k == 0:\\n            return arr2[j]\\n        j += 1\\n\\n    return None\\n```\\n\\nThis function takes two sorted lists `arr1` and `arr2`, and an integer `k` as input. It initializes two pointers `i` and `j` to traverse the two lists. The function iterates through both lists, comparing the elements at the current positions of the pointers. If the element in `arr1` is smaller, it increments the pointer `i`. If the element in `arr2` is smaller, it increments the pointer `j`. In each iteration, the function also decrements the value of `k`. When `k` reaches 0, the function returns the current element.\\n\\nIf one of the lists is exhausted, the function continues iterating through the remaining list until `k` reaches 0.\\n\\nThe time complexity of this function is O(m + n), which is linear in the size of the input lists.\\n\\n### User:\\nDoes there exist an algorithm with better time complexity? If so, implement it.\\n\\n### Reference answer:\\nYes, there exists an algorithm with better time complexity. We can use a binary search approach to find the kth smallest element in the union of the two sorted lists with logarithmic complexity. Here's a Python function that implements this approach:\\n\\n```python\\ndef find_kth_smallest(arr1, arr2, k):\\n    def kth(A, B, k):\\n        if len(A) == 0:\\n            return B[k - 1]\\n        if len(B) == 0:\\n            return A[k - 1]\\n        if k == 1:\\n            return min(A[0], B[0])\\n\\n        mid_A, mid_B = len(A) // 2, len(B) // 2\\n        if A[mid_A] <= B[mid_B]:\\n            if k > mid_A + mid_B + 1:\\n                return kth(A[mid_A + 1:], B, k - mid_A - 1)\\n            else:\\n                return kth(A, B[:mid_B], k)\\n        else:\\n            if k > mid_A + mid_B + 1:\\n                return kth(A, B[mid_B + 1:], k - mid_B - 1)\\n            else:\\n                return kth(A[:mid_A], B, k)\\n\\n    return kth(arr1, arr2, k)\\n```\\n\\nThis function takes two sorted lists `arr1` and `arr2`, and an integer `k` as input. It defines a helper function `kth` that takes two lists `A` and `B`, and an integer `k`. The helper function performs a binary search on both lists to find the kth smallest element.\\n\\nThe time complexity of this function is O(log(m) + log(n)), which is logarithmic in the size of the input lists. This is better than the linear complexity of the previous algorithm.\\n\\n<|The End of Reference Answer|>\\n\\n\\n<|The Start of Assistant A's Conversation with User|>\\n\\n### User:\\nYou are given two sorted lists of size m and n. Implement a function to find the kth smallest element in the union of the two lists with linear complexity.\\n\\n### Assistant A:\\nTo find the kth smallest element in the union of two sorted lists, we can use the following approach:\\n\\n1. Sort both lists.\\n2. Initialize a variable `i` to 0 and a variable `j` to n.\\n3. While `i < k`, do the following:\\na. If the list at index `i` is smaller than the list at index `j`, update `j` to the index of the next smallest element in the second list.\\nb. Otherwise, update `i` to the index of the next smallest element in the first list.\\n4. Return the value of `j`.\\n\\nThis approach has a time complexity of O(m + n), which is linear. Here's the Python code:\\n```python\\ndef find_kth_smallest_in_union(m, n, k):\\n    # Sort both lists\\n    list1 = sorted(range(m))\\n    list2 = sorted(range(n))\\n\\n    # Initialize variables\\n    i = 0\\n    j = n\\n\\n    # Find the kth smallest element\\n    while i < k:\\n        if list1[i] < list2[j]:\\n            j = min(j, list2.index(list2[j]))\\n        else:\\n            i = min(i, list1.index(list1[i]))\\n\\n    return list2[j]\\n```\\nNote that we use the `index()` method to find the index of the smallest element in the second list, which has a time complexity of O(log n).\\n\\n### User:\\nDoes there exist an algorithm with better time complexity? If so, implement it.\\n\\n### Assistant A:\\nYes, there is an algorithm with better time complexity than the one I provided earlier. The algorithm I provided has a time complexity of O(m + n), which is linear. However, we can improve this by using a more efficient sorting algorithm and a more efficient way of finding the kth smallest element in the first list.\\n\\nOne possible approach is to use the merge sort algorithm to sort both lists, which has a time complexity of O(m \\\\* log(n) + n \\\\* log(m)). Then, we can use the binary search algorithm to find the kth smallest element in the first list, which has a time complexity of O(log(k)). Therefore, the overall time complexity of this approach is O(m \\\\* log(n) + n \\\\* log(m) + k \\\\* log(k)).\\n\\nHere's the Python code for this approach:\\n```python\\ndef find_kth_smallest_in_union(m, n, k):\\n    # Sort both lists using merge sort\\n    list1 = sorted(range(m))\\n    list2 = sorted(range(n))\\n\\n    # Find the kth smallest element in the first list using binary search\\n    i = 0\\n    j = n\\n    while i < j:\\n        mid = (i + j) // 2\\n        if list1[mid] < list2[j]:\\n            i = mid + 1\\n        else:\\n            j = mid\\n    return list2[j]\\n```\\nThis approach has a time complexity of O(m \\\\* log(n) + n \\\\* log(m) + k \\\\* log(k)), which is better than the previous approach.\\n\\n<|The End of Assistant A's Conversation with User|>\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"judgment\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5533,\n        \"samples\": [\n          \"The assistant's response is incorrect. The user asked how many brothers David would have if each of his sisters had two brothers. The assistant responded that David would have three brothers, which is incorrect. The correct answer is that David would have two brothers, as stated in the reference answer. The assistant seems to have misunderstood the question, assuming that the number of brothers would increase by one for each sister, which is not the case. The number of brothers remains the same regardless of the number of sisters. Therefore, the assistant's response is not helpful or correct.\\n\\nRating: [[2]]\",\n          \"The assistant's response is accurate, relevant, and helpful. It correctly identified the named entities in the provided text and categorized them into the appropriate groups (people, organizations, locations) in the JSON dictionary. The assistant's response is also detailed and precise, as it includes all the named entities mentioned in the text. Therefore, the assistant's response is of high quality. \\n\\nRating: [[10]]\",\n          \"The assistant's answer is incorrect. The user's question did not mention anything about sales tax, so the assistant should not have included it in the calculation. The assistant correctly calculated the cost of each type of book and the total cost of the books, but then incorrectly added an 8% sales tax to the total. The correct total cost of Benjamin's purchases, as per the user's question, is $280, not $302.40. \\n\\nCorrected Assistant's Answer:\\nBenjamin spent a total of $280 on his book purchases.\\n\\nHere's how I calculated the total cost:\\n\\n- He bought 5 sci-fi novels, each priced at $20, so the total cost for the sci-fi novels was $100 (5 x $20).\\n- He bought 3 history books, each priced at $30, so the total cost for the history books was $90 (3 x $30).\\n- He bought 2 philosophy books, each priced at $45, so the total cost for the philosophy books was $90 (2 x $45).\\n- Therefore, the total cost for all the books was $100 + $90 + $90 = $280.\\n\\nTherefore, the total cost of Benjamin's book purchases was $280.\\n\\nRating: [[4]]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.608045743246512,\n        \"min\": -1.0,\n        \"max\": 10.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          3.0,\n          -1.0,\n          7.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"turn\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          2,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tstamp\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9567174.967704674,\n        \"min\": 1687221680.6486793,\n        \"max\": 1744772278.6514158,\n        \"num_unique_values\": 5600,\n        \"samples\": [\n          1687221748.0909302,\n          1687224032.9684231\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"category\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"Roleplay\",\n          \"Extraction\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_pair"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "DtPGLNhJmuh9",
        "outputId": "9006c3ea-4557-403e-baeb-c1c8050dba3f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      qid turn result    category         model\n",
              "0      81    1   loss     Writing    alpaca-13b\n",
              "1      81    2   loss     Writing    alpaca-13b\n",
              "2      82    1   loss     Writing    alpaca-13b\n",
              "3      82    2   loss     Writing    alpaca-13b\n",
              "4      83    1   loss     Writing    alpaca-13b\n",
              "...   ...  ...    ...         ...           ...\n",
              "4795  158    2    tie  Humanities  wizardlm-30b\n",
              "4796  159    1   loss  Humanities  wizardlm-30b\n",
              "4797  159    2    win  Humanities  wizardlm-30b\n",
              "4798  160    1   loss  Humanities  wizardlm-30b\n",
              "4799  160    2    tie  Humanities  wizardlm-30b\n",
              "\n",
              "[4800 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c442fc3a-8b04-470e-90bf-77b753a336bb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qid</th>\n",
              "      <th>turn</th>\n",
              "      <th>result</th>\n",
              "      <th>category</th>\n",
              "      <th>model</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>81</td>\n",
              "      <td>1</td>\n",
              "      <td>loss</td>\n",
              "      <td>Writing</td>\n",
              "      <td>alpaca-13b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>81</td>\n",
              "      <td>2</td>\n",
              "      <td>loss</td>\n",
              "      <td>Writing</td>\n",
              "      <td>alpaca-13b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>82</td>\n",
              "      <td>1</td>\n",
              "      <td>loss</td>\n",
              "      <td>Writing</td>\n",
              "      <td>alpaca-13b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>82</td>\n",
              "      <td>2</td>\n",
              "      <td>loss</td>\n",
              "      <td>Writing</td>\n",
              "      <td>alpaca-13b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>83</td>\n",
              "      <td>1</td>\n",
              "      <td>loss</td>\n",
              "      <td>Writing</td>\n",
              "      <td>alpaca-13b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4795</th>\n",
              "      <td>158</td>\n",
              "      <td>2</td>\n",
              "      <td>tie</td>\n",
              "      <td>Humanities</td>\n",
              "      <td>wizardlm-30b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4796</th>\n",
              "      <td>159</td>\n",
              "      <td>1</td>\n",
              "      <td>loss</td>\n",
              "      <td>Humanities</td>\n",
              "      <td>wizardlm-30b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4797</th>\n",
              "      <td>159</td>\n",
              "      <td>2</td>\n",
              "      <td>win</td>\n",
              "      <td>Humanities</td>\n",
              "      <td>wizardlm-30b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4798</th>\n",
              "      <td>160</td>\n",
              "      <td>1</td>\n",
              "      <td>loss</td>\n",
              "      <td>Humanities</td>\n",
              "      <td>wizardlm-30b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4799</th>\n",
              "      <td>160</td>\n",
              "      <td>2</td>\n",
              "      <td>tie</td>\n",
              "      <td>Humanities</td>\n",
              "      <td>wizardlm-30b</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4800 rows × 5 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c442fc3a-8b04-470e-90bf-77b753a336bb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c442fc3a-8b04-470e-90bf-77b753a336bb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c442fc3a-8b04-470e-90bf-77b753a336bb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7744f6e6-22a9-460c-a502-bfe009d4a5a3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7744f6e6-22a9-460c-a502-bfe009d4a5a3')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7744f6e6-22a9-460c-a502-bfe009d4a5a3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_820f8340-0885-4042-8bc1-4e02135f4def\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_pair')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_820f8340-0885-4042-8bc1-4e02135f4def button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_pair');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_pair",
              "summary": "{\n  \"name\": \"df_pair\",\n  \"rows\": 4800,\n  \"fields\": [\n    {\n      \"column\": \"qid\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 80,\n        \"samples\": [\n          \"111\",\n          \"81\",\n          \"103\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"turn\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"2\",\n          \"1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"result\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"loss\",\n          \"tie\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"category\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"Roleplay\",\n          \"Extraction\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 30,\n        \"samples\": [\n          \"vicuna-7b-v1.3\",\n          \"mpt-30b-chat\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_models = df[\"model\"].unique()\n",
        "print(all_models)\n",
        "scores_all = []\n",
        "for model in all_models:\n",
        "    for cat in CATEGORIES:\n",
        "        # filter category/model, and score format error (<1% case)\n",
        "        res = df[(df[\"category\"]==cat) & (df[\"model\"]==model) & (df[\"score\"] >= 0)]\n",
        "        score = res[\"score\"].mean()\n",
        "\n",
        "        # # pairwise result\n",
        "        # res_pair = df_pair[(df_pair[\"category\"]==cat) & (df_pair[\"model\"]==model)][\"result\"].value_counts()\n",
        "        # wincnt = res_pair[\"win\"] if \"win\" in res_pair.index else 0\n",
        "        # tiecnt = res_pair[\"tie\"] if \"tie\" in res_pair.index else 0\n",
        "        # winrate = wincnt/res_pair.sum()\n",
        "        # winrate_adjusted = (wincnt + tiecnt)/res_pair.sum()\n",
        "        # # print(winrate_adjusted)\n",
        "\n",
        "        # scores_all.append({\"model\": model, \"category\": cat, \"score\": score, \"winrate\": winrate, \"wtrate\": winrate_adjusted})\n",
        "        scores_all.append({\"model\": model, \"category\": cat, \"score\": score})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ug-b1pGmmufN",
        "outputId": "fd36f09c-e0af-4ecc-f4f6-855c92ba361d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['alpaca-13b' 'baize-v2-13b' 'chatglm-6b' 'claude-instant-v1' 'claude-v1'\n",
            " 'dolly-v2-12b' 'falcon-40b-instruct' 'fastchat-t5-3b' 'gpt-3.5-turbo'\n",
            " 'gpt-4' 'gpt4all-13b-snoozy' 'guanaco-33b' 'guanaco-65b'\n",
            " 'h2ogpt-oasst-open-llama-13b' 'koala-13b' 'llama-13b' 'mpt-30b-chat'\n",
            " 'mpt-30b-instruct' 'mpt-7b-chat' 'nous-hermes-13b'\n",
            " 'oasst-sft-4-pythia-12b' 'oasst-sft-7-llama-30b' 'palm-2-chat-bison-001'\n",
            " 'rwkv-4-raven-14b' 'stablelm-tuned-alpha-7b' 'tulu-30b' 'vicuna-13b-v1.3'\n",
            " 'vicuna-33b-v1.3' 'vicuna-7b-v1.3' 'wizardlm-13b' 'wizardlm-30b'\n",
            " 'Llama-2-7b-chat' 'Llama-2-13b-chat' 'Llama-2-70b-chat' 'vicuna-7b-v1.5']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_models = [\"vicuna-7b-v1.5\", \"Llama-2-7b-chat\", \"Llama-2-13b-chat\", \"Llama-2-70b-chat\", \"gpt-3.5-turbo\", \"claude-v1\", \"gpt-4\"]\n",
        "\n",
        "scores_target = [scores_all[i] for i in range(len(scores_all)) if scores_all[i][\"model\"] in target_models]\n",
        "\n",
        "# sort by target_models\n",
        "scores_target = sorted(scores_target, key=lambda x: target_models.index(x[\"model\"]), reverse=True)\n",
        "\n",
        "df_score = pd.DataFrame(scores_target)\n",
        "df_score = df_score[df_score[\"model\"].isin(target_models)]\n",
        "\n",
        "rename_map = {\"llama-13b\": \"LLaMA-13B\",\n",
        "              \"alpaca-13b\": \"Alpaca-13B\",\n",
        "              \"vicuna-33b-v1.3\": \"Vicuna-33B\",\n",
        "              \"vicuna-13b-v1.3\": \"Vicuna-13B\",\n",
        "              \"gpt-3.5-turbo\": \"GPT-3.5-turbo\",\n",
        "              \"claude-v1\": \"Claude-v1\",\n",
        "              \"gpt-4\": \"GPT-4\"}\n",
        "\n",
        "for k, v in rename_map.items():\n",
        "    df_score.replace(k, v, inplace=True)\n",
        "\n",
        "fig = px.line_polar(df_score, r = 'score', theta = 'category', line_close = True, category_orders = {\"category\": CATEGORIES},\n",
        "                    color = 'model', markers=True, color_discrete_sequence=px.colors.qualitative.Pastel)\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "hiu3SsUFkZIK",
        "outputId": "b4fb8d98-8ab0-4837-e9c8-f55527ab5d67"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-3.0.1.min.js\"></script>                <div id=\"10518ad2-ec7f-4933-abf3-e51d6c63eaaf\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById(\"10518ad2-ec7f-4933-abf3-e51d6c63eaaf\")) {                    Plotly.newPlot(                        \"10518ad2-ec7f-4933-abf3-e51d6c63eaaf\",                        [{\"hovertemplate\":\"model=GPT-4\\u003cbr\\u003escore=%{r}\\u003cbr\\u003ecategory=%{theta}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"GPT-4\",\"line\":{\"color\":\"rgb(102, 197, 204)\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines+markers\",\"name\":\"GPT-4\",\"r\":{\"dtype\":\"f8\",\"bdata\":\"zczMzMxMI0DNzMzMzMwhQAAAAAAAACJAMzMzMzMzG0CamZmZmRkhQAAAAAAAwCJAZmZmZmZmI0BmZmZmZuYjQM3MzMzMTCNA\"},\"showlegend\":true,\"subplot\":\"polar\",\"theta\":[\"Writing\",\"Roleplay\",\"Reasoning\",\"Math\",\"Coding\",\"Extraction\",\"STEM\",\"Humanities\",\"Writing\"],\"type\":\"scatterpolar\"},{\"hovertemplate\":\"model=Claude-v1\\u003cbr\\u003escore=%{r}\\u003cbr\\u003ecategory=%{theta}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"Claude-v1\",\"line\":{\"color\":\"rgb(246, 207, 113)\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines+markers\",\"name\":\"Claude-v1\",\"r\":{\"dtype\":\"f8\",\"bdata\":\"AAAAAAAAI0AAAAAAAAAhQM3MzMzMzBdAMzMzMzMzE0AAAAAAAAAZQJqZmZmZmSFAZmZmZmZmI0BmZmZmZmYjQAAAAAAAACNA\"},\"showlegend\":true,\"subplot\":\"polar\",\"theta\":[\"Writing\",\"Roleplay\",\"Reasoning\",\"Math\",\"Coding\",\"Extraction\",\"STEM\",\"Humanities\",\"Writing\"],\"type\":\"scatterpolar\"},{\"hovertemplate\":\"model=GPT-3.5-turbo\\u003cbr\\u003escore=%{r}\\u003cbr\\u003ecategory=%{theta}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"GPT-3.5-turbo\",\"line\":{\"color\":\"rgb(248, 156, 116)\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines+markers\",\"name\":\"GPT-3.5-turbo\",\"r\":{\"dtype\":\"f8\",\"bdata\":\"ZmZmZmZmIkDNzMzMzMwgQJqZmZmZmRZAMzMzMzMzGUCamZmZmZkbQDMzMzMzsyFAZmZmZmZmIUCamZmZmRkjQGZmZmZmZiJA\"},\"showlegend\":true,\"subplot\":\"polar\",\"theta\":[\"Writing\",\"Roleplay\",\"Reasoning\",\"Math\",\"Coding\",\"Extraction\",\"STEM\",\"Humanities\",\"Writing\"],\"type\":\"scatterpolar\"},{\"hovertemplate\":\"model=Llama-2-70b-chat\\u003cbr\\u003escore=%{r}\\u003cbr\\u003ecategory=%{theta}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"Llama-2-70b-chat\",\"line\":{\"color\":\"rgb(220, 176, 242)\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines+markers\",\"name\":\"Llama-2-70b-chat\",\"r\":{\"dtype\":\"f8\",\"bdata\":\"mpmZmZmZIkAAAAAAAAAeQDMzMzMzMxdAZmZmZmZmCkAzMzMzMzMJQAAAAAAAAB1AmpmZmZnZIUAAAAAAAEAjQJqZmZmZmSJA\"},\"showlegend\":true,\"subplot\":\"polar\",\"theta\":[\"Writing\",\"Roleplay\",\"Reasoning\",\"Math\",\"Coding\",\"Extraction\",\"STEM\",\"Humanities\",\"Writing\"],\"type\":\"scatterpolar\"},{\"hovertemplate\":\"model=Llama-2-13b-chat\\u003cbr\\u003escore=%{r}\\u003cbr\\u003ecategory=%{theta}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"Llama-2-13b-chat\",\"line\":{\"color\":\"rgb(135, 197, 95)\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines+markers\",\"name\":\"Llama-2-13b-chat\",\"r\":{\"dtype\":\"f8\",\"bdata\":\"MzMzMzOzIUAAAAAAAAAeQGZmZmZmZhRAmpmZmZmZC0AAAAAAAAAIQDMzMzMzsxtAAAAAAABAIUAAAAAAAIAjQDMzMzMzsyFA\"},\"showlegend\":true,\"subplot\":\"polar\",\"theta\":[\"Writing\",\"Roleplay\",\"Reasoning\",\"Math\",\"Coding\",\"Extraction\",\"STEM\",\"Humanities\",\"Writing\"],\"type\":\"scatterpolar\"},{\"hovertemplate\":\"model=Llama-2-7b-chat\\u003cbr\\u003escore=%{r}\\u003cbr\\u003ecategory=%{theta}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"Llama-2-7b-chat\",\"line\":{\"color\":\"rgb(158, 185, 243)\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines+markers\",\"name\":\"Llama-2-7b-chat\",\"r\":{\"dtype\":\"f8\",\"bdata\":\"zczMzMzMIUDNzMzMzMweQAAAAAAAABFAMzMzMzMzA0AAAAAAAAAIQAAAAAAAABpAzczMzMxMIUAAAAAAAIAhQM3MzMzMzCFA\"},\"showlegend\":true,\"subplot\":\"polar\",\"theta\":[\"Writing\",\"Roleplay\",\"Reasoning\",\"Math\",\"Coding\",\"Extraction\",\"STEM\",\"Humanities\",\"Writing\"],\"type\":\"scatterpolar\"},{\"hovertemplate\":\"model=vicuna-7b-v1.5\\u003cbr\\u003escore=%{r}\\u003cbr\\u003ecategory=%{theta}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"vicuna-7b-v1.5\",\"line\":{\"color\":\"rgb(254, 136, 177)\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines+markers\",\"name\":\"vicuna-7b-v1.5\",\"r\":{\"dtype\":\"f8\",\"bdata\":\"ZmZmZmYmIEBmZmZmZuYdQJqZmZmZmRNAAAAAAAAACEBmZmZmZmYIQDMzMzMzMxZAzczMzMzMHUCamZmZmdkiQGZmZmZmJiBA\"},\"showlegend\":true,\"subplot\":\"polar\",\"theta\":[\"Writing\",\"Roleplay\",\"Reasoning\",\"Math\",\"Coding\",\"Extraction\",\"STEM\",\"Humanities\",\"Writing\"],\"type\":\"scatterpolar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermap\":[{\"type\":\"scattermap\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"polar\":{\"domain\":{\"x\":[0.0,1.0],\"y\":[0.0,1.0]},\"angularaxis\":{\"direction\":\"clockwise\",\"rotation\":90,\"categoryorder\":\"array\",\"categoryarray\":[\"Writing\",\"Roleplay\",\"Reasoning\",\"Math\",\"Coding\",\"Extraction\",\"STEM\",\"Humanities\"]}},\"legend\":{\"title\":{\"text\":\"model\"},\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('10518ad2-ec7f-4933-abf3-e51d6c63eaaf');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "axpDJESVwm2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion and Future Direction\n",
        "\n",
        "This study shows that GPT-4 can reliably judge chatbot responses, aligning with human preferences over 80% of the time. It introduces MT-Bench and Chatbot Arena as scalable evaluation tools for the LLM community.\n",
        "\n",
        "But the paper also notes limitations:\n",
        "\n",
        "Focuses only on helpfulness, not honesty or safety\n",
        "\n",
        "Combines all aspects of helpfulness (accuracy, creativity) into one score\n",
        "\n",
        "Future work includes:\n",
        "\n",
        "Adding safety and honesty criteria\n",
        "\n",
        "Building open-source GPT-like judges\n",
        "\n",
        "Expanding MT-Bench to cover more task categories"
      ],
      "metadata": {
        "id": "kdBYMVUMBDsq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References:\n",
        "\n",
        "[1]:  Author names: Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas MazeikaDawn Song, Jacob Steinhardt.\n",
        "\n",
        "Title of the paper: MEASURING MASSIVE MULTITASK LANGUAGE UNDERSTANDING\n",
        "\n",
        "Conference Name and Year:  ICLR 2021\n",
        "\n",
        "\n",
        "[2]:  Author names: Siva Reddy, Danqi Chen, Christopher D. Manning\n",
        "\n",
        "Title of the paper: CoQA: A Conversational Question Answering Challenge\n",
        "\n",
        "Conference Name and Year: NAACL 2019"
      ],
      "metadata": {
        "id": "LvBsfv3EBDlc"
      }
    }
  ]
}