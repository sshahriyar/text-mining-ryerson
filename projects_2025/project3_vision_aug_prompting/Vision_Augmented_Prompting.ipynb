{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4hFxY3FEWvc"
      },
      "source": [
        "# Title: Enhancing LLM Reasoning via Vision-Augmented Prompting\n",
        "\n",
        "#### Members' Names or Individual's Name: Ashish Sunuwar, Yahya Shaikh\n",
        "\n",
        "####  Emails: ashish.sunuwar@torontomu.ca , yahya.shaikh@torontomu.ca\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2KlKp7wEWvd"
      },
      "source": [
        "# Introduction:\n",
        "\n",
        "#### Problem Description:\n",
        "Reasoning about complex tasks like geometry intersections, time series forecasting, or visual puzzles requires a combination of verbal logic and spatial intuition. Take a geometry problem, for example: “How many times do a circle and a line intersect?” Or a time series question like: “What comes next in this pattern?” While these seem simple for humans — who often sketch or visualize before making decisions — language models process only text. This gap in spatial reasoning limits their ability to solve such problems accurately.\n",
        "\n",
        "\n",
        "#### Context of the Problem:\n",
        "\n",
        "Large Language Models (LLMs) like GPT-4 have made impressive advances in language-based tasks. But when it comes to problems that require spatial understanding, temporal trends, or structural insight, they fall short. These are tasks where humans rely on visual thinking — sketching, drawing patterns, or mentally simulating movement — all of which are missing in traditional LLM pipelines. Without a way to visualize or “see” the problem, LLMs can’t reason with the same depth as humans.\n",
        "\n",
        "#### Limitation About other Approaches:\n",
        "\n",
        "Most existing LLM-based reasoning methods — like standard prompting, Chain-of-Thought (CoT), and even Self-Consistent CoT (CoT-SC) — rely entirely on text. These methods work well when the answer is just a matter of logical inference or memory, but they break down in cases where visual cues are essential. For example, in geometry problems, models often hallucinate or underestimate intersections because they can’t “see” the shapes. Similarly, time series predictions using only text struggle to capture visual or cyclical patterns in data.\n",
        "\n",
        "\n",
        "#### Solution:\n",
        "\n",
        "To bridge this gap, we implemented a vision-augmented approach. Instead of asking LLMs to solve complex problems blindly, we give them eyes. We use external tools to draw diagrams or visualize patterns described in the problem text, and then feed these images back into the model alongside the original prompt. Our Vision-Augmented Prompting (VAP) pipeline walks the model through a 3-stage process — planning, iterative visual reasoning, and final conclusion — to improve decision-making in both geometry and time-based tasks.\n",
        "\n",
        "![Difference between the non-VAP and VAP approach](fig_comparison.png \"Difference between the non-VAP and VAP approach Ref[2], https://github.com/Ashish-Sunuwar/Vision_Augmented_Prompting/blob/main/src/Images/fig_comparison.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFPcNdgbEWve"
      },
      "source": [
        "# Background\n",
        "\n",
        "Explain the related work using the following table\n",
        "\n",
        "| Reference | Explanation | Dataset/Input | Weakness |\n",
        "| --- | --- | --- | --- |\n",
        "| Wei et al. [1] | Introduced Chain-of-thought prompting, where LLMs reason step-by-step before producing an answer. This improved performance on logical reasoning and math word problems. | Diverse reasoning benchmarks (e.g., GSM8K, SVAMP) | Fails on tasks that require visual or spatial reasoning, text-only steps can't represent diagrams or physical relationships. |\n",
        "| Xiao et al. [2] | Proposed Vision-Augmented Prompting (VAP), a multimodal reasoning framework. Combines image synthesis (e.g., matplotlib) with LLM planning, iterative visual reasoning, and final conclusion — enabling reasoning over geometry, time series, Sudoku, and TSP. | Geometry tasks from BIG-bench, Darts time series, synthetic Sudoku and TSP samples | Current VAP is training-free and general, but lacks dynamic image updates, uses only static renderers (e.g., matplotlib). Future work could focus on image feedback loops, dynamic drawing, and interpretability. |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FR3c8FnOEWve"
      },
      "source": [
        "# Methodology\n",
        "\n",
        "Our project simplifies and implements the VAP framework for two reasoning domains: geometry intersection problems and time series forecasting. For keeping it simple we will just go through the geometry intersection problem. The same three-stage process is used in both cases, reflecting how humans often work: plan, think through steps with a visual aid, then conclude.\n",
        "\n",
        "Stage 1: Planning\n",
        "We begin by asking the model to generate a structured plan. This includes:\n",
        "*   What tool to use for drawing (we used Matplotlib)\n",
        "*   How to set up the drawing canvas (e.g., axis limits)\n",
        "*   What sequence of steps to take when thinking through the problem\n",
        "*   When to stop drawing or reasoning\n",
        "\n",
        "\n",
        "The plan is returned in structured JSON format and helps the model establish a “mental” framework before solving the task.\n",
        "\n",
        "Stage 2: Iterative Reasoning\n",
        "Once the problem is visualized and the plan is ready, we ask the model to think step-by-step. At each iteration, it updates its thoughts (e.g., “Adding a circle intersects with the line here…”) and refines its mental image. While we do not update the image in real-time, this mimics how a human would mentally update their sketch as they add more information.\n",
        "\n",
        "Stage 3: Conclusive Reasoning\n",
        "\n",
        "Finally, the model is given the image, the original problem, and the reasoning trail. It now makes a final judgment — such as how many intersection points exist, or what the next value in a series should be. This combination of visual + verbal thought gives better accuracy compared to text-only reasoning.\n",
        "\n"
   
        
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CPbu6-pEWvf"
      },
      "source": [
        "# Implementation\n",
        "We used a dataset of natural-language geometry problems from the BIG-bench benchmark that was provided by the paper. Each problem describes shapes — circles, lines, polygons — and asks how many intersections occur.\n",
        "\n",
        "Our system:\n",
        "\n",
        "\n",
        "*   Parsed the shapes using regex from the problem text\n",
        "\n",
        "*   Drew them using Matplotlib to create a visual diagram\n",
        "\n",
        "*   Used call_gpt() to query the model with: Standard prompt and Chain-of-Thought prompt\n",
        "\n",
        "*   Vision-Augmented prompt (3 stages)\n",
        "*   Logged predictions, matched them against ground truth, and tracked accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjEH0ivhEWvf"
      },
      "outputs": [],
      "source": [
        "# Importing all necessary libraries\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# GPT API call function\n",
        "from call_gpt import call_gpt\n",
        "#Prompts used for standard and COT prompting strategies\n",
        "from prompts.intersection_count import standard_prompt, cot_prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vap1XGlzEWvf"
      },
      "outputs": [],
      "source": [
        "# Clean GPT responses by removing markdown fences and trimming whitespaces\n",
        "def clean_response(response_text):\n",
        "    cleaned = response_text.strip()\n",
        "    if cleaned.startswith(\"```\"):\n",
        "        lines = cleaned.splitlines()\n",
        "        if lines and lines[0].startswith(\"```\"):\n",
        "            lines.pop(0)\n",
        "        if lines and lines[-1].startswith(\"```\"):\n",
        "            lines.pop()\n",
        "        if lines and lines and lines[0].strip().lower() == \"json\":\n",
        "            lines.pop(0)\n",
        "        cleaned = \"\\n\".join(lines).strip()\n",
        "    return cleaned\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhH6fVo9EWvf"
      },
      "outputs": [],
      "source": [
        "# This function runs standard, chain-of-thought prompts\n",
        "def run_intersection_count(dataset_dir='dataset', task='intersection_geometry', model='gpt-4o-mini', method='standard', k_samples=5, log_dir='log'):\n",
        "    from prompts.intersection_count import standard_prompt, cot_prompt\n",
        "\n",
        "    # Create a unique logging folder for this run\n",
        "    method = method.lower()\n",
        "    log_dir_base = os.path.join(log_dir, task)\n",
        "    os.makedirs(log_dir_base, exist_ok=True)\n",
        "    current_time_str = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    run_dir = os.path.join(log_dir_base, f'{model}_{method}_{current_time_str}')\n",
        "    os.makedirs(run_dir, exist_ok=True)\n",
        "\n",
        "    # Set up a CSV file to store results\n",
        "    result_path = os.path.join(run_dir, 'result.csv')\n",
        "    result_file = open(result_path, 'w', encoding='utf8')\n",
        "    result_file.write('problem_id,pred_answer,is_correct,num_shape\\n')\n",
        "    result_file.flush()\n",
        "\n",
        "    # Utility function to check correctness\n",
        "    def check_identity(gt, pred):\n",
        "        try:\n",
        "            return int(pred) == int(gt)\n",
        "        except:\n",
        "            return False\n",
        "\n",
        "    # Load the dataset containing geometry problems\n",
        "    metadata_path = os.path.join(dataset_dir, task, 'task.json')\n",
        "    with open(metadata_path, 'r', encoding='utf8') as f:\n",
        "        metadata = json.load(f)\n",
        "\n",
        "    right_count = 0\n",
        "    current_count = 0\n",
        "\n",
        "    # Loop through each geometry problem\n",
        "    for question_id, item in enumerate(tqdm(metadata[:120])):\n",
        "        log_path = os.path.join(run_dir, f'{question_id}.log')\n",
        "        log_file = open(log_path, 'w', encoding='utf8')\n",
        "\n",
        "        temperature = 0.7\n",
        "        if method == 'standard':\n",
        "            prompt = standard_prompt\n",
        "            max_tokens = 150\n",
        "        elif method == 'cot':\n",
        "            prompt = cot_prompt\n",
        "            max_tokens = 2048\n",
        "        elif method == 'cot-sc':\n",
        "            prompt = cot_prompt\n",
        "            max_tokens = 512\n",
        "\n",
        "        # Call GPT with formatted prompt\n",
        "        result = call_gpt(prompt.format(problem=item['input']), model=model, temperature=temperature, max_tokens=max_tokens)\n",
        "        print(result)\n",
        "\n",
        "        # Extract answer from GPT's response\n",
        "        if method == 'standard':\n",
        "            try:\n",
        "                pred_answer = int(result)\n",
        "            except:\n",
        "                pred_answer = 0\n",
        "        elif method == 'cot':\n",
        "            try:\n",
        "                m = re.findall(r'answer:\\s*(\\d+)', result.lower())[0]\n",
        "                pred_answer = int(m)\n",
        "            except:\n",
        "                pred_answer = 0\n",
        "        elif method == 'cot-sc':\n",
        "            count_map = {}\n",
        "            for _ in range(k_samples):\n",
        "                try:\n",
        "                    m = re.findall(r'answer:\\s*(\\d+)', result.lower())[0]\n",
        "                    pred_answer = int(m)\n",
        "                except:\n",
        "                    pred_answer = 0\n",
        "                count_map[pred_answer] = count_map.get(pred_answer, 0) + 1\n",
        "            pred_answer = sorted(count_map.items(), key=lambda x: x[1], reverse=True)[0][0]\n",
        "\n",
        "        # Log result for this question\n",
        "        log_file.write('=' * 40 + '\\n')\n",
        "        log_file.write(f'problem: {item[\"input\"]}\\n')\n",
        "        log_file.write(f'result: {result}\\n')\n",
        "        log_file.write(f'pred_answer: {pred_answer}\\n')\n",
        "        log_file.write(f'ground truth: {item[\"answer\"]}\\n')\n",
        "        log_file.flush()\n",
        "\n",
        "        # Accuracy tracking\n",
        "        is_correct = check_identity(item['answer'], pred_answer)\n",
        "        if is_correct:\n",
        "            right_count += 1\n",
        "        current_count += 1\n",
        "        result_file.write(f'{current_count-1},{pred_answer},{is_correct},{item[\"num_shape\"]}\\n')\n",
        "        result_file.flush()\n",
        "        print(f'Accuracy: {right_count / current_count * 100:.2f}%')\n",
        "\n",
        "        log_file.close()\n",
        "        time.sleep(5)\n",
        "\n",
        "    # Final accuracy summary\n",
        "    print(f'Final Accuracy: {right_count / len(metadata) * 100:.2f}%')\n",
        "    with open(os.path.join(run_dir, 'summary.log'), 'w', encoding='utf8') as f:\n",
        "        f.write(f'Accuracy: {right_count / len(metadata) * 100:.2f}%\\n')\n",
        "    result_file.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gpq9KgjhEo5H"
      },
      "outputs": [],
      "source": [
        "# Parse geometry descriptions to extract circles, lines, and polygons\n",
        "def parse_geometry_description(problem_text):\n",
        "    shapes = []\n",
        "    text_lower = problem_text.lower()\n",
        "    circle_matches = re.findall(r'circle centered at \\(([-\\d\\.]+),\\s*([-\\d\\.]+)\\)\\s*with radius\\s*([-\\d\\.]+)', text_lower)\n",
        "    for (cx, cy, r_str) in circle_matches:\n",
        "        shapes.append((\"circle\", (float(cx), float(cy)), float(r_str.rstrip('.'))))\n",
        "    line_matches = re.findall(r'line segment from \\(([-\\d\\.]+),\\s*([-\\d\\.]+)\\)\\s*to\\s*\\(([-\\d\\.]+),\\s*([-\\d\\.]+)\\)', text_lower)\n",
        "    for (x1, y1, x2, y2) in line_matches:\n",
        "        shapes.append((\"line\", (float(x1), float(y1)), (float(x2), float(y2))))\n",
        "    poly_matches = re.findall(r'polygon with coordinates\\s*\\[([^\\]]+)\\]', text_lower)\n",
        "    for coords_str in poly_matches:\n",
        "        pts = re.findall(r'\\(([-\\d\\.]+),\\s*([-\\d\\.]+)\\)', coords_str)\n",
        "        if pts:\n",
        "            shapes.append((\"polygon\", [(float(px), float(py)) for px, py in pts]))\n",
        "    return shapes\n",
        "\n",
        "# Draw and save the figure using matplotlib based on parsed geometry shapes\n",
        "def draw_geometry(shapes, out_filename=\"figure.png\"):\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set_xlim(-10, 10)\n",
        "    ax.set_ylim(-10, 10)\n",
        "    for shape in shapes:\n",
        "        if shape[0] == \"circle\":\n",
        "            (cx, cy), r = shape[1], shape[2]\n",
        "            ax.add_patch(plt.Circle((cx, cy), r, fill=False, color='b'))\n",
        "        elif shape[0] == \"line\":\n",
        "            (x1, y1), (x2, y2) = shape[1], shape[2]\n",
        "            ax.plot([x1, x2], [y1, y2], color='r')\n",
        "        elif shape[0] == \"polygon\":\n",
        "            pts = shape[1] + [shape[1][0]]\n",
        "            ax.plot([p[0] for p in pts], [p[1] for p in pts], color='g')\n",
        "    plt.title(\"Visualized Geometry Problem\")\n",
        "    plt.savefig(out_filename)\n",
        "    plt.close(fig)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6t8c2-jZEpFh"
      },
      "outputs": [],
      "source": [
        "# Vision-Augmented Prompting (VAP) 3-stage pipeline for solving geometry intersection problems\n",
        "def run_intersection_count_vap(dataset_dir=\"dataset\", task=\"intersection_geometry\", model=\"gpt-4o-mini\", log_dir=\"log\", max_tokens=500):\n",
        "    # Set up directories for logs and figures\n",
        "    current_time_str = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    log_dir_base = os.path.join(log_dir, task + '_vap_separate')\n",
        "    os.makedirs(log_dir_base, exist_ok=True)\n",
        "    run_dir = os.path.join(log_dir_base, f'{model}_separate_{current_time_str}')\n",
        "    os.makedirs(run_dir, exist_ok=True)\n",
        "    figures_dir = os.path.join(run_dir, 'figures')\n",
        "    os.makedirs(figures_dir, exist_ok=True)\n",
        "\n",
        "    # CSV to save predictions\n",
        "    result_csv = os.path.join(run_dir, 'result.csv')\n",
        "    result_file = open(result_csv, 'w', encoding='utf8')\n",
        "    result_file.write('problem_id,plan,iterations,final_answer,is_correct,num_shape\\n')\n",
        "    result_file.flush()\n",
        "\n",
        "    # Stage 1: High-level plan for drawing\n",
        "    def planning_stage(problem_text):\n",
        "        prompt = f\"\"\"Your task is to visualize a geometry problem by creating an image.\n",
        "Before drawing, provide a high-level plan including:\n",
        "- Tool Selection\n",
        "- Initialization\n",
        "- Iterative Drawing Approach\n",
        "Output JSON with: tool, initialization, iterative_approach, termination_condition.\n",
        "Problem Description: {problem_text}\"\"\"\n",
        "        return json.loads(clean_response(call_gpt(prompt, model=model, temperature=0, max_tokens=max_tokens)))\n",
        "\n",
        "    # Stage 2: Simulate drawing updates and thoughts\n",
        "    def iterative_reasoning_stage(problem_text, image_path, plan):\n",
        "        plan_str = json.dumps(plan)\n",
        "        prompt = f\"\"\"You are a visualizer for this geometry problem:\n",
        "{problem_text}\n",
        "Plan: {plan_str}\n",
        "Image: {image_path}\n",
        "Simulate iterative drawing steps in JSON array (iteration, thought, iterative_draw_step).\"\"\"\n",
        "        return json.loads(clean_response(call_gpt(prompt, model=model, temperature=0, max_tokens=max_tokens, image_path=image_path)))\n",
        "\n",
        "    # Stage 3: Predict the final answer using the problem text, image and reasoning trajectory\n",
        "    def conclusive_reasoning_stage(problem_text, image_path, plan, iterations):\n",
        "        prompt = f\"\"\"Based on:\n",
        "- Problem: {problem_text}\n",
        "- Plan: {json.dumps(plan)}\n",
        "- Iterations: {json.dumps(iterations)}\n",
        "- Image: {image_path}\n",
        "Output final answer JSON as: {{ \"final_answer\": X }}\"\"\"\n",
        "        return int(json.loads(clean_response(call_gpt(prompt, model=model, temperature=0, max_tokens=max_tokens, image_path=image_path))).get(\"final_answer\", 0))\n",
        "\n",
        "    # Load dataset and run all 3 stages\n",
        "    metadata_path = os.path.join(dataset_dir, task, 'task.json')\n",
        "    with open(metadata_path, 'r', encoding='utf8') as f:\n",
        "        metadata = json.load(f)\n",
        "\n",
        "    right_count = 0\n",
        "    for idx, item in enumerate(metadata[:120]):\n",
        "        problem_text = item[\"input\"]\n",
        "        gt_answer = int(item[\"answer\"])\n",
        "        num_shape = item.get(\"num_shape\", 0)\n",
        "        figure_path = os.path.join(figures_dir, f\"fig_{idx}.png\")\n",
        "        draw_geometry(parse_geometry_description(problem_text), figure_path)\n",
        "\n",
        "        try:\n",
        "            plan = planning_stage(problem_text)\n",
        "            iters = iterative_reasoning_stage(problem_text, figure_path, plan)\n",
        "            final_ans = conclusive_reasoning_stage(problem_text, figure_path, plan, iters)\n",
        "        except Exception as e:\n",
        "            print(f\"[{idx}] Error: \", e)\n",
        "            plan, iters, final_ans = {}, [], 0\n",
        "\n",
        "        correct = int(final_ans) == gt_answer\n",
        "        right_count += int(correct)\n",
        "        result_file.write(f\"{idx},{json.dumps(plan)},{json.dumps(iters)},{final_ans},{correct},{num_shape}\\n\")\n",
        "        result_file.flush()\n",
        "        print(f\"[{idx}] Pred: {final_ans} | GT: {gt_answer} | Correct: {correct}\")\n",
        "\n",
        "    # Print and log final results\n",
        "    acc = right_count / len(metadata[:120]) * 100\n",
        "    print(f\"Final Accuracy: {acc:.2f}%\")\n",
        "    with open(os.path.join(run_dir, 'summary.log'), 'w') as f:\n",
        "        f.write(f\"Accuracy: {acc:.2f}%\\n\")\n",
        "    result_file.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWdcfewvEuGh"
      },
      "outputs": [],
      "source": [
        "# Run Standard, COT and VAP methods\n",
        "run_intersection_count(dataset_dir='dataset', method='standard', model='gpt-4o-mini')\n",
        "run_intersection_count(dataset_dir='dataset', method='cot', model='gpt-4o-mini')\n",
        "run_intersection_count_vap(dataset_dir='dataset', task='intersection_geometry', model='gpt-4o-mini', log_dir='log')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Bih3VQPpT7D"
      },
      "source": [
        "# Results\n",
        "\n",
        "####Image formation:\n",
        "####Geometry Problem\n",
        "![Geometry Problem Image Formation](fig_geometry.png \"Geometry Problem Image Formation, https://github.com/Ashish-Sunuwar/Vision_Augmented_Prompting/blob/main/src/Images/fig_geometry.png\")\n",
        "\n",
        "####Time Series Problem\n",
        "![Time Series Problem Image Formation](fig_time.png \"Time Series Problem Image Formation, https://github.com/Ashish-Sunuwar/Vision_Augmented_Prompting/blob/main/src/Images/fig_time.png\")\n",
        "\n",
        "\n",
        "####Final Results:\n",
        "####Geometry Problem\n",
        "![Geometry Problem Results](fig_geometry_result.png \"Geometry Problem Results, https://github.com/Ashish-Sunuwar/Vision_Augmented_Prompting/blob/main/src/Images/fig_geometry_result.png\")\n",
        "\n",
        "####Time Series Problem\n",
        "![Time Series Problem Results](fig_time_result.png \"Time Series Problem Results, https://github.com/Ashish-Sunuwar/Vision_Augmented_Prompting/blob/main/src/Images/fig_time_result.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOCEXeUkEWvf"
      },
      "source": [
        "# Conclusion and Future Direction\n",
        "\n",
        "This project taught us how essential visual thinking is for solving problems beyond pure text. Vision-Augmented Prompting enables LLMs to reason more like humans — by looking, planning, and thinking step by step. Even with a basic setup using Matplotlib, we observed meaningful accuracy gains. We saw this was especially helpful for problems that require spatial reasoning, like geometry intersection counting, and temporal understanding, like time series forecasting.\n",
        "\n",
        "Future work could include exploring its application in more diverse set of problems, enabling dynamic image updates giving live visual feedback to the model and improving interpretabilily for the model reasoning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFrdsWb-EWvg"
      },
      "source": [
        "# References:\n",
        "\n",
        "[1]:  Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi,\n",
        "Quoc V. Le, and Denny Zhou, Chain-of-thought prompting elicits reasoning in large language\n",
        "models, NeurIPS, 2022\n",
        "\n",
        "[2]:  Ziyang Xiao, Dongxiang Zhang, Xiongwei Han, Xiaojin Fu, Wing Yin Yu, Tao Zhong, SaiWu, Yuan Wang, Jianwei Yin, Gang Chen, Enhancing LLM Reasoning via Vision-Augmented\n",
        "Prompting, NeurIPS, 2024"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
