{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb-pNC-YzFFz"
      },
      "source": [
        "# Title: MediQ: Question-Asking LLMs and a Benchmark for Reliable Interactive Clinical Reasoning\n",
        "\n",
        "#### Members' Names or Individual's Name: Muhammad Maqsud Hussain, Beha Shirshekar\n",
        "\n",
        "####  Emails:m5hussain@torontomu.ca, beha.shirshekar@torontomu.ca"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JtM0NVDzFF0"
      },
      "source": [
        "# Introduction:\n",
        "\n",
        "#### Problem Description:\n",
        "\n",
        "The paper discusses the limitations of large language models (LLMs) in high-stakes decision-making scenarios, such as clinical environments, where gathering missing information is crucial. Standard question-answering (QA) tasks assume all necessary information is provided upfront, but real-world scenarios require follow-up questions and reasoning. There had been gaps found for such models that are designed for proactive inquiry.\n",
        "\n",
        "#### Context of the Problem:\n",
        "\n",
        "The problem of proactive information-seeking in LLMs is important because real-world decision-making, particularly in high-stakes environments like healthcare, requires more than just answering questions—it demands an investigative approach. Standard LLMs assume all necessary information is provided upfront, but in reality, professionals often work with incomplete data and must ask follow-up questions to make informed decisions.\n",
        "\n",
        "Without this capability, LLMs risk making incorrect or even harmful recommendations when key details are missing. For example, in a clinical setting, an AI assistant that fails to ask about patient history, symptoms, or risk factors could lead to misdiagnosis or inappropriate treatment. This is equally critical in other domains, such as law, finance, and cybersecurity, where incomplete information can lead to poor decision-making with significant consequences. Improving information-seeking behavior in LLMs is essential to making AI systems more reliable, safe, and useful in complex real-world applications.\n",
        "\n",
        "#### Limitation About other Approaches:\n",
        "\n",
        "standard medical question-answering (QA) tasks are formulated in a single-turn setup where all necessary information is provided upfront, and\n",
        "the model is not expected to interact with users.\n",
        "\n",
        "#### Solution:\n",
        "\n",
        "The method in the paper provides a pipeline to convert single-turn medical benchmarks into an interactive format. In this way, it will interactively collect missing information to more accurate decision."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNNIQ6RdzFF1"
      },
      "source": [
        "# Background\n",
        "\n",
        "Explain the related work using the following table\n",
        "\n",
        "| Reference |Explanation |  Dataset/Input |Weakness\n",
        "| --- | --- | --- | --- |\n",
        "| Jin et al. [1] | They  implemented both rule-based and popular neural methods by sequentially combining a document retriever and a machine comprehension model.| OpenQA dataset  | cannot achieve good performance on these data |\n",
        "| Yasunaga et al.[2] | They proposed a self-supervised approach to pretraining a deeply joint language-knowledge foundation model from text and KG at scale| ConceptNet knowledge graph as well as CommonsenseQA, OpenBookQA and RiddleSense datasets |It is an encoder model (analogous to BERT) and does not perform language generation |\n",
        "Wu et al.[3] | They established a scalable pipeline to construct a large-scale medical visual question-answering dataset| PMC-VQA dataset | Potential distribution biases inlcuded in the dataset |\n",
        "Johri et al.[4] | They introduced the Conversational Reasoning Assessment Framework for Testing in Medicine (CRAFT-MD), a novel approach for evaluating clinical LLMs| nejm_imagechallenge_dataset | More effective and ethical incorporation of LLMs into the medical domain |\n",
        "Li et al.[5] | The authors proposed to change the static paradigm to an interactive one, develop systems that proactively ask questions to gather more information and respond reliably, and introduce an benchmark—MEDIQ—to evaluate question-asking ability in LLMs| MEDQA and CRAFT-MD datasets | The scarcity of datasets that contain detailed patient information sufficient for a medical diagnosis. Thus future work can focus on collecting a rich dataset in open-ended medical consultations|\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methodology\n",
        "\n",
        "This paper introduces the MEDIQ benchmark, a framework designed to evaluate the question-asking abilities of Large Language Models (LLMs) in interactive clinical reasoning scenarios. It addresses the limitations of existing static, single-turn medical question-answering (QA) benchmarks, which do not reflect real-world clinical interactions where information is often incomplete and requires follow-up questions.\n",
        "The MEDIQ framework simulates a dynamic medical consultation between two key components:\n",
        "\n",
        "**Patient System: **This system simulates a human patient and has access to the entire patient record. It is designed to respond to the Expert system's follow-up questions with coherent and factual answers consistent with the patient's information. The Patient system's reliability is crucial and is evaluated based on factuality (whether responses are faithful to the patient record) and relevance (whether the response answers the expert's question).\n",
        "  \n",
        "**Expert System: **This system acts as a doctor's assistant and is presented with an initial, potentially incomplete patient description. The core task of the Expert system is to decide, at each turn, whether it has enough information to make a confident medical decision. If unconfident, it should ask information-seeking follow-up questions to gather missing details. The performance of the Expert system is evaluated on the efficiency of the conversation (number of follow-up questions asked) and the accuracy of the final diagnosis.\n",
        "MEDIQ-Expert system is broken down into five modular, medically-grounded steps :\n",
        "![Alternate text ](fig1.png \"Expert system information flow breakdown.\")\n",
        "\n",
        "*1.Initial Assessment:* Provides an initial understanding of the patient's condition and potential knowledge gaps.\n",
        "\n",
        "*2.Abstention:* Evaluates the Expert system's confidence level to decide whether to answer or ask a question. Different abstention strategies are introduced, including BASIC, Numerical, Binary, Scale, Rationale Generation (RG), and Self-Consistency (SC).\n",
        "\n",
        "*3.Question Generation:* Crafts information-seeking questions to elicit additional medical evidence when more information is needed.\n",
        "\n",
        "*4.Information Integration:* Aggregates all gathered patient information to update the understanding of the patient's condition.\n",
        "\n",
        "*5.Decision Making:* Provides the final answer to the medical question when enough evidence is gathered .\n",
        "\n",
        "**Expert System Variants with Different Abstention Strategies:**\n",
        "The paper introduces several variants, each employing different strategies to probe the LLM's confidence. Here are the different abstention strategies discussed:\n",
        "\n",
        "*BASIC: *This serves as a baseline where the model implicitly indicates its decision by either generating a question or providing an answer to the multiple-choice question (MCQ).\n",
        "\n",
        "*Numerical:* This strategy explicitly asks the model to generate a numerical confidence score between 0 and 1. A predefined threshold is then used to decide whether to ask a question or provide an answer.\n",
        "\n",
        "*Binary:* This approach simplifies confidence assessment by asking the model to classify whether enough information is present to answer the question with a \"YES\" or \"NO\".\n",
        "\n",
        "*Scale:* To introduce more granularity, this method uses a 5-point Likert scale with defined confidence levels (e.g., \"Very Confident\", \"Somewhat Confident\"). The model selects a rating, and a threshold determines the subsequent action.\n",
        "\n",
        "*Rationale Generation (RG):* This technique prompts the model to generate a reasoning chain explaining its confidence level. This longer context window can improve the model's ability to assess its knowledge gaps, and this strategy is applied to the Numerical, Binary, and Scale abstention prompts. The rationale is also included in the context for subsequent question generation.\n",
        "\n",
        "*Self-Consistency (SC):* To further enhance the abstaining decision, Self-Consistency is applied to the above variants. This involves prompting the LLM multiple times and then taking the average (for Numerical and Scale) or the mode (for Binary) of the outputs as the final decision. Interestingly, the results showed that self-consistency alone could degrade performance unless combined with rationale generation.\n",
        "\n",
        "**Experiments:**\n",
        "Now, we will introduce the experimental setup aimed at validating each component of MEDIQ. It evaluates the Patient system first, followed by establishing the relationship between information availability and accuracy, and finally, improving the information-seeking ability of LLMs within the MEDIQ framework. The evaluation datasets used are iMEDQA and iCRAFT-MD, which are interactive versions of the MEDQA and CRAFT-MD medical QA datasets, created by providing only partial initial information to the Expert system.\n",
        "\n",
        "***1.Patient System Reliability Evaluation:***\n",
        "\n",
        "The Patient system is evaluated based on two key metrics: Factuality Score (whether the responses are consistent with the patient record) and Relevance Score (whether the responses answer the Expert's question).\n",
        "\n",
        "Three Patient system variants were tested: Direct, Instruct, and Fact-Select. The results, presented in Table 1, show that Fact-Select, which decomposes the patient record into atomic facts, significantly improves both factuality and relevance compared to the Direct and Instruct methods. The authors conclude that using atomic facts as units of information reduces hallucinations and improves the reliability of the Patient system. The Fact-Select setting is used for the Patient system in all subsequent experiments.\n",
        "\n",
        "| Model|Factuality|Relevance\n",
        "| ---  | ---  | ---\n",
        "Direct | 55.9 | 75.5 |\n",
        "Instruct | 62.8 | 78.6 |\n",
        "Fact-Select | 89.1 | 79.9 |\n",
        "**Table 1**\n",
        "\n",
        "***2.Expert System Experiments:***\n",
        "The Expert system aims to simulate the medical decision-making process of clinicians who seek additional patient information.\n",
        "\n",
        "*a.Benchmarking Existing LLMs in Incomplete Information Scenarios:*\n",
        "Here the performance of non-interactive Expert systems (vanilla LLMs) with varying levels of initial information availability to establish baselines are evaluated and  the relationship between information and accuracy is observed. The models (Llama-3, GPT-3.5, GPT-4) were tested under three conditions: Full (all patient information provided upfront), Initial (only age, gender, and chief complaint), and None (only the MCQ). The results demonstrate a significant drop in accuracy as less information is available. Furthermore, when LLMs are given the option to ask questions in the BASIC interactive setup, their accuracy is even lower than in the non-interactive Initial setting, indicating that adapting LLMs to proactive information-seeking is nontrivial.\n",
        "\n",
        "*b.Interactive Expert Systems:*\n",
        "Now, interactive Expert systems with different abstention strategies to improve their information-seeking ability is evaluated. A baseline BASIC Expert system was created by prompting LLMs to either ask a question or make a decision at each turn. To improve performance, various abstention strategies were implemented in the Abstention Module, which decides when to ask questions based on the model's confidence. These strategies include Numerical, Binary, and Scale confidence scoring, along with techniques like Rationale Generation (RG) and Self-Consistency (SC). The results show that integrating a dedicated Abstention Module significantly enhances performance over the BASIC setup. The best performance (BEST system) is achieved by Scale Abstention with Rationale Generation and Self-Consistency, which outperforms the BASIC interactive setup and the non-interactive Initial setup.\n",
        "\n"
      ],
      "metadata": {
        "id": "4SzmW_r4bNj9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vuI-qQg_c4b9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMCQ-CckzFF1"
      },
      "source": [
        "# Implementation\n",
        "\n",
        "In this section, you will provide the code and its explanation. You may have to create more cells after this. (To keep the Notebook clean, do not display debugging output or thousands of print statements from hundreds of epochs. Make sure it is readable for others by reviewing it yourself carefully.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "5Zxkc1b1z4rE",
        "outputId": "dda9ad07-0058-45ed-ab66-b2f281cfed2b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'gr' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-8d7a945dcd6f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Gradio chat interface\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m gr.ChatInterface(\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mfn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgpt_response\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"🧠 MediQ GPT-4 Clinical Reasoning\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'gr' is not defined"
          ]
        }
      ],
      "source": [
        "def gpt_response(message, history=[]):\n",
        "    # Reformat history into proper OpenAI chat message format\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a careful and experienced clinical reasoning expert.\"}\n",
        "    ]\n",
        "    for user_msg, bot_msg in history:\n",
        "        messages.append({\"role\": \"user\", \"content\": user_msg})\n",
        "        messages.append({\"role\": \"assistant\", \"content\": bot_msg})\n",
        "    messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    # Call OpenAI GPT-3.5\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        temperature=0.3,\n",
        "        max_tokens=300\n",
        "    )\n",
        "\n",
        "    reply = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "    return reply\n",
        "\n",
        "# Gradio chat interface\n",
        "gr.ChatInterface(\n",
        "    fn=gpt_response,\n",
        "    title=\"🧠 MediQ GPT-4 Clinical Reasoning\",\n",
        "    description=\"Ask a clinical question. GPT-4 will simulate adaptive expert reasoning.\",\n",
        ").launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWQdTanBzFF3"
      },
      "source": [
        "# Conclusion and Future Direction\n",
        "\n",
        "In this project, we implemented the MediQ benchmark to explore how LLMs can simulate interactive clinical reasoning by asking follow-up questions or making diagnostic decisions. We deployed the system using Hugging Face Spaces and integrated OpenAI’s API, gaining hands-on experience with real-time AI applications.\n",
        "\n",
        "The results showed that LLMs can generate relevant clinical questions, but their responses are sometimes generic or lack diagnostic precision. Limitations include API constraints, lack of domain fine-tuning, and no built-in clinical validation.\n",
        "\n",
        "Future improvements could involve using medical-specific models, adding clinician feedback, and enhancing the model’s decision-making accuracy. This project deepened my understanding of AI's role and challenges in clinical reasoning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bka9JD5UzFF3"
      },
      "source": [
        "# References:\n",
        "\n",
        "[1]:  Di Jin ,Eileen Pan,Nassim Oufattole,Wei-Hung Weng,Hanyi Fang,Peter Szolovits, *\"What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams\"*, 2021\n",
        "\n",
        "[2]:  Michihiro Yasunaga, Antoine Bosselut, Hongyu Ren, Xikun Zhang, Christopher D Manning, Percy Liang, Jure Leskovec, *\"Deep Bidirectional Language-Knowledge Graph Pretraining\"*, 2022, AArXiv. https://arxiv.org/abs/2210.09338\n",
        "\n",
        "\n",
        "[3]:  Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya Zhang, Yanfeng Wang, *\"PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering\"*, 2023, ArXiv. https://arxiv.org/abs/2305.10415\n",
        "\n",
        "[4]:  Shreya Johri, Jaehwan Jeong, Benjamin A. Tran, Daniel I. Schlessinger, Shannon Wongvibulsin, Zhuo Ran Cai, Roxana Daneshjou,  Pranav Rajpurkar, *\"Guidelines For Rigorous Evaluation of Clinical LLMs For Conversational Reasoning*\", 2024, 10.1101/2023.09.12.23295399\n",
        "\n",
        "[5]: Shuyue Stella Li, Vidhisha Balachandran, Shangbin Feng, Jonathan S. Ilgen, Emma Pierson, Pang Wei Koh, Yulia Tsvetkov, MediQ: *\"Question-Asking LLMs and a Benchmark for Reliable Interactive Clinical Reasoning*\", 2024, ArXiv. https://arxiv.org/abs/2406.00922"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}